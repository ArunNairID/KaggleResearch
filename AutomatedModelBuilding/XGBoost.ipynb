{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBoost</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# scoring functions for feature selection\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XGBoost():\n",
    "    \"\"\"A class to choose and train an XGBoost model\"\"\"\n",
    "    \"\"\"This implements the XGBoost model\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./processed_data/', regressor=False, n_jobs=1,\n",
    "                 criterion=None, random_state=1, n_estimators=1000,\n",
    "                 opt_func=None, inv_opt_func=None, scorer=None):\n",
    "        \"\"\"Initializes the XGBoost class\"\"\"\n",
    "        \n",
    "        # TODO:\n",
    "        #    - Create an initialization function that will allow for\n",
    "        #      a saved model to be loaded\n",
    "        \n",
    "        self.n_jobs = n_jobs\n",
    "        self.regressor = regressor\n",
    "        self.data_dir = data_dir\n",
    "        self.regressor = regressor\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "        self.opt_func = opt_func\n",
    "        self.inv_opt_func = inv_opt_func\n",
    "        self.scorer = scorer\n",
    "        \n",
    "        # keep track of which functions have been called\n",
    "        self.read_data_called = False\n",
    "        self.select_features_called = False\n",
    "        self.tune_params_called = False\n",
    "        self.train_model_called = False\n",
    "        \n",
    "        if criterion == None:\n",
    "            if regressor:\n",
    "                self.criterion = 'reg:linear'\n",
    "            else:\n",
    "                self.criterion = 'binary:logistic'\n",
    "        \n",
    "    def read_data(self):\n",
    "        \"\"\"Read in the data from the specified directory\"\"\"\n",
    "        \n",
    "        self.read_data_called = True\n",
    "        \n",
    "        self.cv_X_df = pd.read_csv(self.data_dir+'cv_X.csv', header=0)\n",
    "        self.cv_y_df = pd.read_csv(self.data_dir+'cv_y.csv', header=0)\n",
    "        self.cv_ids_df = pd.read_csv(self.data_dir+'cv_ids.csv', header=0)\n",
    "        self.train_X_df = pd.read_csv(self.data_dir+'train_X.csv', header=0)\n",
    "        self.train_y_df = pd.read_csv(self.data_dir+'train_y.csv', header=0)\n",
    "        self.train_ids_df = pd.read_csv(self.data_dir+'train_ids.csv', header=0)\n",
    "        self.test_X_df = pd.read_csv(self.data_dir+'test_X.csv', header=0)\n",
    "        self.test_ids_df = pd.read_csv(self.data_dir+'test_ids.csv', header=0)\n",
    "        \n",
    "    def get_feature_importances(self):\n",
    "        \"\"\"Trains a basic random forest to get a list of feature importances\"\"\"\n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "        \n",
    "        importance_tree = RandomForestRegressor(n_jobs=self.n_jobs,\n",
    "                                random_state=self.random_state,\n",
    "                                n_estimators=self.n_estimators)\n",
    "        \n",
    "        # train a basic model so that we can access feature importances\n",
    "        cv_X = self.cv_X_df.values\n",
    "        cv_y = np.ravel(self.cv_y_df.values)\n",
    "        importance_tree.fit(cv_X, cv_y)\n",
    "        self.feature_importances = importance_tree.feature_importances_\n",
    "        \n",
    "        if not(self.opt_func == None):\n",
    "            cv_y = self.opt_func(cv_y)\n",
    "        \n",
    "        # sort the features by importances, most important first\n",
    "        self.sorted_features = self.feature_importances.argsort()[::-1]\n",
    "        \n",
    "        return self.sorted_features\n",
    "        \n",
    "    '''    \n",
    "    def select_features(self):\n",
    "        \"\"\"Select features from the dataset by using feature importances\n",
    "        to add features one by one for selection\"\"\"\n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "            \n",
    "        self.select_features_called = True\n",
    "            \n",
    "        # TODO:\n",
    "        #   - VERIFY THAT THIS IS WORKING\n",
    "        #   - THINK OF OTHER TYPES OF FEATURE SELECTION\n",
    "        #     BACKWARD FEATURE SELECTION?\n",
    "        #   - IS THIS THE ONLY CONSIDERATION FOR SELECTING FEATURES??\n",
    "        #     WHAT IF THE SCORE IS VERY HIGH WITH FEW FEATURES OUT OF MANY??\n",
    "        #     WHAT IF THE LARGEST SCORE IS AFTER MANY FEATURES ARE ADDED\n",
    "        #     WITH MINIMAL IMPROVEMENT TO SCORE??\n",
    "        \n",
    "        if self.regressor:\n",
    "            model = RandomForestRegressor(oob_score=True,\n",
    "                                        n_jobs=self.n_jobs,\n",
    "                                        random_state=self.random_state,\n",
    "                                        n_estimators=self.n_estimators,\n",
    "                                        criterion=self.criterion)\n",
    "        else:\n",
    "            model = RandomForestClassifier(oob_score=True,\n",
    "                                         n_jobs=self.n_jobs,\n",
    "                                         random_state=self.random_state,\n",
    "                                         n_estimators=self.n_estimators)\n",
    "            \n",
    "        # keep track of the scores from cross validation\n",
    "        all_scores = []\n",
    "        max_score = -10000\n",
    "        early_stopping_rounds = 10\n",
    "        early_stopping_count = 0\n",
    "        \n",
    "        # which features to add, in order\n",
    "        self.sorted_features = self.get_feature_importances()\n",
    "        \n",
    "        cv_X = self.cv_X_df.values\n",
    "        cv_y = np.ravel(self.cv_y_df.values)\n",
    "        cv_X_selected = np.array([])\n",
    "        cv_X_selected.shape = (cv_X.shape[0], 0)\n",
    "                \n",
    "        if not(self.opt_func == None):\n",
    "            cv_y = self.opt_func(cv_y)\n",
    "            \n",
    "        for feature in self.sorted_features:\n",
    "            \n",
    "            # format data so that it can be added easily\n",
    "            added_feat = np.transpose(cv_X[:, feature])\n",
    "            added_feat.shape += (1, )\n",
    "            \n",
    "            # append data to the next column\n",
    "            cv_X_selected = np.append(cv_X_selected, added_feat, axis=1)\n",
    "            \n",
    "            # train a model with the augmented data\n",
    "            #model.fit(cv_X_selected, cv_y)\n",
    "            #score = model.oob_score_\n",
    "            scores = cross_val_score(model, cv_X_selected, cv_y, cv=3, scoring='neg_mean_squared_error')\n",
    "            score = scores.mean()\n",
    "            print(feature)\n",
    "            print(score)\n",
    "            \n",
    "            # have an evaluation metric to measure performance of added features\n",
    "            all_scores.append(score)\n",
    "            \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                early_stopping_count = 0\n",
    "            else:\n",
    "                early_stopping_count += 1\n",
    "                \n",
    "            if early_stopping_count > early_stopping_rounds:\n",
    "                break\n",
    "        \n",
    "        max_score_index = np.argmax(all_scores)\n",
    "        print(all_scores)\n",
    "        print(max_score_index)\n",
    "        \n",
    "        # select the features\n",
    "        self.selected_feature_indices = self.sorted_features[:(max_score_index+1)]\n",
    "        self.selected_features = cv_X_selected[:, :(max_score_index+1)]\n",
    "        \n",
    "        return self.selected_feature_indices\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def select_features_2(self, score_func_name='f_regression', percentage=100):\n",
    "        \"\"\"Function to select features based on sklearn scoring\"\"\"\n",
    "        \n",
    "        self.select_features_called = True\n",
    "        \n",
    "        cv_X = self.cv_X_df.values\n",
    "        cv_y = np.ravel(self.cv_y_df.values)\n",
    "        \n",
    "        if score_func_name == 'chi2':\n",
    "            score_func = chi2\n",
    "        elif score_func_name == 'f_regression':\n",
    "            score_func = f_regression\n",
    "        elif score_func_name == 'f_classif':\n",
    "            score_func = f_classif\n",
    "        \n",
    "        if not(self.opt_func == None):\n",
    "            cv_y = self.opt_func(cv_y)\n",
    "        \n",
    "        self.feature_selector = SelectPercentile(score_func, percentile=percentage)\n",
    "        self.selected_features = self.feature_selector.fit_transform(cv_X, cv_y)\n",
    "        \n",
    "        return self.selected_features\n",
    "        \n",
    "    def tune_params(self):\n",
    "        \"\"\"Function to handle tuning hyperparameters of the model\"\"\"\n",
    "        \n",
    "        # TODO:\n",
    "        #   - FIND GOOD VALUES OF PARAMETERS TO TEST\n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "        \n",
    "        self.tune_params_called = True\n",
    "        \n",
    "        # tree structure\n",
    "        self.tune_max_depth = True\n",
    "        self.tune_min_child_weight = True\n",
    "        \n",
    "        self.tune_gamma = True\n",
    "        \n",
    "        self.tune_subsample = True\n",
    "        self.tune_colsample_bytree = True\n",
    "        \n",
    "        self.tune_reg_alpha = True\n",
    "        self.tune_reg_lambda = True\n",
    "        \n",
    "        # Parameters to tune for XGBoost\n",
    "        # the maximum number of features to be considered for a tree.\n",
    "        #max_depth_pos = [2, 4, 6, 8, 10, 12, 20]\n",
    "        #min_child_weight_pos = [1, 2, 4, 6, 8, 10]\n",
    "        max_depth_pos = [6, 10, 20]\n",
    "        min_child_weight_pos = [1, 3, 5]\n",
    "        \n",
    "        #gamma_pos = [0, 0.001, 0.01, 0.1, 1]\n",
    "        gamma_pos = [0, 0.01, 1]\n",
    "        \n",
    "        #subsample_pos = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        #colsample_bytree_pos = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        subsample_pos = [0.5, 0.75, 1]\n",
    "        colsample_bytree_pos = [0.5, 0.75, 1]\n",
    "        \n",
    "        #reg_alpha_pos = [0, 0.001, 0.01, 0.1, 1]\n",
    "        #reg_lambda_pos = [0, 0.001, 0.01, 0.1, 1]        \n",
    "        reg_alpha_pos = [0, 0.01, 1]\n",
    "        reg_lambda_pos = [0, 0.01, 1]\n",
    "        \n",
    "        # TODO:\n",
    "        #    - FIGURE OUT WHICH OTHER PARAMETERS TO TUNE\n",
    "        #    - DETERMINE GOOD VALUES TO TEST\n",
    "        \n",
    "        # set these parameters to commonly used starting values\n",
    "        self.max_depth=5 # default=3\n",
    "        learning_rate_tune=0.1\n",
    "        self.silent=False\n",
    "        self.gamma=0\n",
    "        self.min_child_weight=1\n",
    "        self.max_delta_step=0\n",
    "        self.subsample=0.8 #default=1\n",
    "        self.colsample_bytree=0.8 # default=1\n",
    "        self.colsample_bylevel=1\n",
    "        self.reg_alpha=0\n",
    "        self.reg_lambda=1\n",
    "        self.scale_pos_weight=1\n",
    "        \n",
    "        # training parameters\n",
    "        early_stopping_rounds = 20\n",
    "        n_estimators_tune = self.n_estimators\n",
    "        cv_splits = 5\n",
    "\n",
    "        \n",
    "        # only take the selected features if we have performed training already\n",
    "        if self.select_features_called:\n",
    "            cv_X = self.selected_features\n",
    "        else:\n",
    "            cv_X = self.cv_X_df.values\n",
    "        \n",
    "        cv_y = np.ravel(self.cv_y_df.values)\n",
    "        \n",
    "        if not(self.opt_func == None):\n",
    "            cv_y = self.opt_func(cv_y)\n",
    "        \n",
    "        if self.regressor:\n",
    "            model = xgb.XGBRegressor(max_depth=self.max_depth,\n",
    "                                learning_rate=learning_rate_tune,\n",
    "                                n_estimators=n_estimators_tune,\n",
    "                                silent=self.silent,\n",
    "                                objective=self.criterion,\n",
    "                                nthread=self.n_jobs,\n",
    "                                gamma=self.gamma,\n",
    "                                min_child_weight=self.min_child_weight,\n",
    "                                max_delta_step=self.max_delta_step,\n",
    "                                subsample=self.subsample,\n",
    "                                colsample_bytree=self.colsample_bytree,\n",
    "                                colsample_bylevel=self.colsample_bylevel,\n",
    "                                reg_alpha=self.reg_alpha,\n",
    "                                reg_lambda=self.reg_lambda,\n",
    "                                scale_pos_weight=self.scale_pos_weight,\n",
    "                                seed=self.random_state)\n",
    "        else:\n",
    "            model = xgb.XGBClassifier(max_depth=self.max_depth,\n",
    "                                learning_rate=learning_rate_tune,\n",
    "                                n_estimators=n_estimators_tune,\n",
    "                                silent=self.silent,\n",
    "                                objective=self.criterion,\n",
    "                                nthread=self.n_jobs,\n",
    "                                gamma=self.gamma,\n",
    "                                min_child_weight=self.min_child_weight,\n",
    "                                max_delta_step=self.max_delta_step,\n",
    "                                subsample=self.subsample,\n",
    "                                colsample_bytree=self.colsample_bytree,\n",
    "                                colsample_bylevel=self.colsample_bylevel,\n",
    "                                reg_alpha=self.reg_alpha,\n",
    "                                reg_lambda=self.reg_lambda,\n",
    "                                scale_pos_weight=self.scale_pos_weight,\n",
    "                                seed=self.random_state)\n",
    "                    \n",
    "        # perform training first to get a baseline and number\n",
    "        # of training rounds\n",
    "        # GET EARLY STOPPING TO WORK FOR THIS\n",
    "        #model.fit(cv_X, cv_y)\n",
    "        #n_estimators_tune = model.best_iteration\n",
    "        #best_score = model.best_score\n",
    "        model.set_params(n_estimators=n_estimators_tune)\n",
    "        \n",
    "        \n",
    "        # perform parameter-spac e searching\n",
    "        if self.tune_max_depth and self.tune_min_child_weight:\n",
    "            scores = []\n",
    "            params = []\n",
    "            for max_depth in max_depth_pos:\n",
    "                model.set_params(max_depth=max_depth)\n",
    "                for min_child_weight in min_child_weight_pos:\n",
    "                    model.set_params(min_child_weight=min_child_weight)\n",
    "                    print(model.get_params)\n",
    "                    score = cross_val_score(model, cv_X, cv_y, cv=cv_splits,\n",
    "                                         scoring=make_scorer(self.scorer))\n",
    "                    score = score.mean()\n",
    "                    scores.append(score)\n",
    "                    params.append((max_depth, min_child_weight))\n",
    "                    print(score)\n",
    "            \n",
    "            # select these indices\n",
    "            best_param_index = np.argmax(scores)\n",
    "            print(best_param_index)\n",
    "            self.max_depth = params[best_param_index][0]\n",
    "            self.min_child_weight = params[best_param_index][1]\n",
    "            model.set_params(max_depth=self.max_depth)\n",
    "            model.set_params(min_child_weight=self.min_child_weight)\n",
    "            \n",
    "        if self.tune_gamma:\n",
    "            scores = []\n",
    "            params = []\n",
    "            for gamma in gamma_pos:\n",
    "                model.set_params(gamma=gamma)\n",
    "                print(model.get_params)\n",
    "                score = cross_val_score(model, cv_X, cv_y, cv=cv_splits,\n",
    "                                     scoring=make_scorer(self.scorer))\n",
    "                score = score.mean()\n",
    "                scores.append(score)\n",
    "                params.append(gamma)\n",
    "                print(score)\n",
    "            \n",
    "            best_param_index = np.argmax(scores)\n",
    "            print(best_param_index)\n",
    "            self.gamma = params[best_param_index]\n",
    "            model.set_params(gamma=self.gamma)\n",
    "                    \n",
    "        if self.tune_subsample and self.tune_colsample_bytree:\n",
    "            scores = []\n",
    "            params = []\n",
    "            for subsample in subsample_pos:\n",
    "                model.set_params(subsample=subsample)\n",
    "                for colsample_bytree in colsample_bytree_pos:\n",
    "                    model.set_params(colsample_bytree=colsample_bytree)\n",
    "                    print(model.get_params)\n",
    "                    score = cross_val_score(model, cv_X, cv_y, cv=cv_splits,\n",
    "                                         scoring=make_scorer(self.scorer))\n",
    "                    score = score.mean()\n",
    "                    scores.append(score)\n",
    "                    params.append((subsample, colsample_bytree))\n",
    "                    print(score)\n",
    "            \n",
    "            best_param_index = np.argmax(scores)\n",
    "            print(best_param_index)\n",
    "            self.subsample = params[best_param_index][0]\n",
    "            self.colsample_bytree = params[best_param_index][1]\n",
    "            model.set_params(subsample=self.subsample)\n",
    "            model.set_params(colsample_bytree=self.colsample_bytree)\n",
    "\n",
    "        if self.tune_reg_alpha and self.tune_reg_lambda:\n",
    "            scores = []\n",
    "            params = []\n",
    "            for reg_alpha in reg_alpha_pos:\n",
    "                model.set_params(reg_alpha=reg_alpha)\n",
    "                for reg_lambda in reg_lambda_pos:\n",
    "                    model.set_params(reg_lambda=reg_lambda)\n",
    "                    print(model.get_params)\n",
    "                    score = cross_val_score(model, cv_X, cv_y, cv=cv_splits,\n",
    "                                         scoring=make_scorer(self.scorer))\n",
    "                    score = score.mean()\n",
    "                    scores.append(score)\n",
    "                    params.append((reg_alpha, reg_lambda))\n",
    "                    print(score)\n",
    "            \n",
    "            best_param_index = np.argmax(scores)\n",
    "            print(best_param_index)\n",
    "            self.reg_alpha = params[best_param_index][0]\n",
    "            self.reg_labmda = params[best_param_index][1]\n",
    "            model.set_params(reg_alpha=self.reg_alpha)\n",
    "            model.set_params(reg_lambda=self.reg_lambda)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def train_model(self, n_estimators=10000, learning_rate=0.01):\n",
    "        \"\"\"Trains the model on all training data\n",
    "        and returns a model to be used for prediction\"\"\"\n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "            \n",
    "        self.train_model_called = True\n",
    "        \n",
    "        \n",
    "            \n",
    "        # determine which model to use\n",
    "        if self.tune_params_called:\n",
    "            self.model = self.model\n",
    "            self.model.set_params(n_estimators=n_estimators)\n",
    "        else:\n",
    "            if self.regressor:\n",
    "                model = xgb.XGBRegressor(max_depth=self.max_depth,\n",
    "                                learning_rate=learning_rate,\n",
    "                                n_estimators=n_estimators,\n",
    "                                silent=self.silent,\n",
    "                                objective=self.criterion,\n",
    "                                nthread=self.n_jobs,\n",
    "                                gamma=self.gamma,\n",
    "                                min_child_weight=self.min_child_weight,\n",
    "                                max_delta_step=self.max_delta_step,\n",
    "                                subsample=self.subsample,\n",
    "                                colsample_bytree=self.colsample_bytree,\n",
    "                                colsample_bylevel=self.colsample_bylevel,\n",
    "                                reg_alpha=self.reg_alpha,\n",
    "                                reg_lambda=self.reg_lambda,\n",
    "                                scale_pos_weight=self.scale_pos_weight,\n",
    "                                seed=self.random_state)\n",
    "            else:\n",
    "                model = xgb.XGBClassifier(max_depth=self.max_depth,\n",
    "                                learning_rate=learning_rate,\n",
    "                                n_estimators=n_estimators,\n",
    "                                silent=self.silent,\n",
    "                                objective=self.criterion,\n",
    "                                nthread=self.n_jobs,\n",
    "                                gamma=self.gamma,\n",
    "                                min_child_weight=self.min_child_weight,\n",
    "                                max_delta_step=self.max_delta_step,\n",
    "                                subsample=self.subsample,\n",
    "                                colsample_bytree=self.colsample_bytree,\n",
    "                                colsample_bylevel=self.colsample_bylevel,\n",
    "                                reg_alpha=self.reg_alpha,\n",
    "                                reg_lambda=self.reg_lambda,\n",
    "                                scale_pos_weight=self.scale_pos_weight,\n",
    "                                seed=self.random_state)\n",
    "        \n",
    "        if self.select_features_called:\n",
    "            train_X = self.selected_features\n",
    "        else:\n",
    "            train_X = self.train_X_df.values\n",
    "            \n",
    "        train_y = np.ravel(self.train_y_df.values)\n",
    "                        \n",
    "        if not(self.opt_func == None):\n",
    "            train_y = self.opt_func(train_y)\n",
    "            \n",
    "        print(self.model.get_params)\n",
    "        \n",
    "        self.model.fit(train_X, train_y)\n",
    "        \n",
    "    def predict_output(self):\n",
    "        \"\"\"Make predictions on test data.\"\"\"\n",
    "        \n",
    "        if not(self.train_model_called):\n",
    "            raise AssertionError(\"Train the model first!\")\n",
    "        \n",
    "        # transform the features to match the selected features from training\n",
    "        #test_X_all = self.test_X_df.values\n",
    "        #test_X = np.array([])\n",
    "        #test_X.shape = (test_X_all.shape[0], 0)\n",
    "        #for feature in self.selected_feature_indices:\n",
    "            #\n",
    "            # format data so that it can be added easily\n",
    "            #added_feat = np.transpose(test_X_all[:, feature])\n",
    "            #added_feat.shape += (1, )\n",
    "            #\n",
    "            # append data to the next column\n",
    "            #test_X = np.append(test_X, added_feat, axis=1)\n",
    "            \n",
    "        \n",
    "        test_X = self.test_X_df.values\n",
    "        test_X = self.feature_selector.transform(test_X)\n",
    "        \n",
    "        self.pred = self.model.predict(test_X)\n",
    "                        \n",
    "        if not(self.inv_opt_func == None):\n",
    "            self.pred = self.inv_opt_func(self.pred)\n",
    "            \n",
    "        return self.pred\n",
    "    \n",
    "    def write_output(self, output_file='output.csv', header=['Id', 'Result'],\n",
    "                    out_dir='./output/'):\n",
    "        # create output dir if it does not exist\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            \n",
    "        # write output to a file\n",
    "        prediction_file = open(out_dir+output_file, 'w')\n",
    "        open_file_object = csv.writer(prediction_file, lineterminator='\\n')\n",
    "        open_file_object.writerow(header)\n",
    "        data = zip(np.ravel(self.test_ids_df.values), self.pred)\n",
    "        open_file_object.writerows(data)\n",
    "        prediction_file.close()\n",
    "        \n",
    "    def get_model(self):\n",
    "        \"\"\"Return the model used for prediction\"\"\"\n",
    "        return self.model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_rmse(y, y_pred):\n",
    "    return -1*np.sqrt(np.mean((y_pred-y)**2))\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    correct = np.sum([1 if y[x] == y_pred[x] else 0 for x in range(len(y))])\n",
    "    return float(correct) / len(y)\n",
    "\n",
    "def log_e(y):\n",
    "    return np.log(y)\n",
    "\n",
    "def log_10(y):\n",
    "    return np.log10(y)\n",
    "\n",
    "def exp_e(y):\n",
    "    return np.exp(y)\n",
    "\n",
    "def exp_10(y):\n",
    "    return np.power(10, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:279: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= row_norms(X.T)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/usr/local/lib/python3.5/dist-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.930666864608\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=3, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.929785818524\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=5, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.928833573499\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.95278577998\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=3, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.951642956632\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
      "       min_child_weight=5, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.950000184274\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.957881106752\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=3, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.955404666483\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=5, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.952714181349\n",
      "6\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.957881106752\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.958142988862\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=1, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.8)>\n",
      "0.957000001134\n",
      "1\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.5)>\n",
      "0.956262336648\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.5)>\n",
      "0.955143093256\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.5)>\n",
      "0.9529526422\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.75)>\n",
      "0.959690642293\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.75)>\n",
      "0.958833371627\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=0.75)>\n",
      "0.955261834849\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.962309727139\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.96009556286\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.953571188192\n",
      "6\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=0,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965048131778\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=0.01,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.964833729847\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.962309727139\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=0,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.964905172576\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=0.01,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.964809889088\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.962167088244\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=1, reg_lambda=0,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.960095775554\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=1, reg_lambda=0.01,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.959881588973\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=20, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=1, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "0.957476801131\n",
      "0\n",
      "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=3,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=1, silent=False, subsample=1)>\n",
      "[2 0 9 ..., 3 9 2]\n"
     ]
    }
   ],
   "source": [
    "model = XGBoost(n_jobs=3, regressor=False, criterion='binary:logistic', \n",
    "                    opt_func=None, inv_opt_func=None, scorer=accuracy,\n",
    "                    n_estimators=20)\n",
    "#criterion: binary:logistic or reg:linear\n",
    "\n",
    "model.read_data()\n",
    "\n",
    "features = model.select_features_2(score_func_name='f_regression', percentage=100)\n",
    "\n",
    "model.tune_params()\n",
    "\n",
    "model.train_model(n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "pred = model.predict_output()\n",
    "print(pred)\n",
    "\n",
    "model.write_output(output_file='mnist_output_XGBoost.csv', header=['DigitId', 'Label'])\n",
    "\n",
    "train_model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
