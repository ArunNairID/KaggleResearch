{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \"\"\"A class to choose and train a random forest model\"\"\"\n",
    "    \"\"\"This implements the sklearn Random Forest Model\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./processed_data/', regressor=False, n_jobs=1):\n",
    "        \"\"\"Initializes the random forest class\"\"\"\n",
    "        \n",
    "        # Create an initialization function that will allow for\n",
    "        # a saved model to be loaded\n",
    "        \n",
    "        self.n_jobs = n_jobs\n",
    "        self.regressor = regressor\n",
    "        self.data_dir = data_dir\n",
    "        self.regressor = regressor\n",
    "        \n",
    "        if self.regressor:\n",
    "            self.model = RandomForestRegressor()\n",
    "        else:\n",
    "            self.model = RandomForestClassifier()\n",
    "        \n",
    "        # *******************************************\n",
    "        # USE THESE IN FUNCTIONS TO KEEP TRACK OF WHAT \n",
    "        # HAS ALREADY BEEN DONE TO DATA, MODELS, ETC...\n",
    "        # keep track of which functions have been called\n",
    "        self.read_data_called = False\n",
    "        self.select_features_called = False\n",
    "        self.tune_params_called = False\n",
    "        self.train_model_called = False\n",
    "        # ********************************************\n",
    "        \n",
    "    def read_data(self):\n",
    "        \"\"\"Read in the data from the specified directory\"\"\"\n",
    "        \n",
    "        self.read_data_called = True\n",
    "        \n",
    "        self.cv_X_df = pd.read_csv(self.data_dir+'cv_X.csv', header=0, index_col=0)\n",
    "        self.cv_X = self.cv_X_df.values\n",
    "        self.cv_y_df = pd.read_csv(self.data_dir+'cv_y.csv', header=0, index_col=0)\n",
    "        self.cv_y = self.cv_y_df.values\n",
    "        self.cv_ids_df = pd.read_csv(self.data_dir+'cv_ids.csv', header=0, index_col=0)\n",
    "        self.train_X_df = pd.read_csv(self.data_dir+'train_X.csv', header=0, index_col=0)\n",
    "        self.train_y_df = pd.read_csv(self.data_dir+'train_y.csv', header=0, index_col=0)\n",
    "        self.train_ids_df = pd.read_csv(self.data_dir+'train_ids.csv', header=0, index_col=0)\n",
    "        self.test_X_df = pd.read_csv(self.data_dir+'test_X.csv', header=0, index_col=0)\n",
    "        self.test_ids_df = pd.read_csv(self.data_dir+'test_ids.csv', header=0, index_col=0)\n",
    "        \n",
    "    def get_feature_importances(self):\n",
    "        \"\"\"Trains a basic random forest to get a list of feature importances\"\"\"\n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "        \n",
    "        importance_tree = RandomForestRegressor(n_jobs=1,\n",
    "                                random_state=1,\n",
    "                                n_estimators=100,\n",
    "                                max_features='sqrt',\n",
    "                                max_depth=10)\n",
    "        \n",
    "        # train a basic model so that we can access feature importances\n",
    "        cv_X = self.cv_X_df.values\n",
    "        cv_y = np.ravel(self.cv_y_df.values)\n",
    "        importance_tree.fit(cv_X, cv_y)\n",
    "        self.feature_importances = importance_tree.feature_importances_\n",
    "        \n",
    "        # sort the features by importances, most important first\n",
    "        self.sorted_features = self.feature_importances.argsort()[::-1]\n",
    "        \n",
    "        return self.sorted_features\n",
    "            \n",
    "    def select_features(self):\n",
    "        \"\"\"Select features from the dataset by using feature importances\n",
    "        to add features one by one for selection\"\"\"\n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "            \n",
    "        self.select_features_called = True\n",
    "            \n",
    "        # TODO:\n",
    "        #   - VERIFY THAT THIS IS WORKING\n",
    "        #   - THINK OF OTHER TYPES OF FEATURE SELECTION\n",
    "        #     BACKWARD FEATURE SELECTION?\n",
    "        \n",
    "        if self.regressor:\n",
    "            model = RandomForestRegressor(oob_score=True,\n",
    "                                        n_jobs=self.n_jobs,\n",
    "                                        random_state=1,\n",
    "                                        n_estimators=1000)\n",
    "        else:\n",
    "            model = RandomForestClassifier(oob_score=True,\n",
    "                                         n_jobs=self.n_jobs,\n",
    "                                         random_state=1,\n",
    "                                         n_estimators=1000)\n",
    "            \n",
    "        # keep track of the scores from cross validation\n",
    "        all_scores = []\n",
    "        \n",
    "        # which features to add, in order\n",
    "        self.sorted_features = self.get_feature_importances()\n",
    "        \n",
    "        cv_X = self.cv_X_df.values\n",
    "        cv_y = np.ravel(self.cv_y_df.values)\n",
    "        cv_X_selected = cv_X[:, self.sorted_features[:1]]\n",
    "        for feature in self.sorted_features[1:]:\n",
    "            \n",
    "            # format data so that it can be added easily\n",
    "            added_feat = np.transpose(cv_X[:, feature])\n",
    "            added_feat.shape += (1, )\n",
    "            \n",
    "            # append data to the next column\n",
    "            cv_X_selected = np.append(cv_X_selected, added_feat, axis=1)\n",
    "            \n",
    "            # train a model with the augmented data\n",
    "            model.fit(cv_X_selected, cv_y)\n",
    "            score = model.oob_score_\n",
    "            #scores = cross_val_score(feature_select_model,\n",
    "            #                       cv_X_selected, cv_y, cv=2)\n",
    "            #score = scores.mean()\n",
    "            \n",
    "            # have an evaluation metric to measure performance of added features\n",
    "            all_scores.append(score)\n",
    "        \n",
    "        # **************************************************************\n",
    "        # IS THIS THE ONLY CONSIDERATION FOR SELECTING FEATURES??\n",
    "        # WHAT IF THE SCORE IS VERY HIGH WITH FEW FEATURES OUT OF MANY??\n",
    "        # WHAT IF THE LARGEST SCORE IS AFTER MANY FEATURES ARE ADDED\n",
    "        # WITH MINIMAL IMPROVEMENT TO SCORE??\n",
    "        max_score_index = np.argmax(all_scores)\n",
    "        # *************************************************************\n",
    "        \n",
    "        self.selected_feature_indices = self.sorted_features[:(max_score_index+1)]\n",
    "        self.selected_features = cv_X_selected[:, :max_score_index+1]\n",
    "        # **************************************************************\n",
    "        \n",
    "        return self.selected_feature_indices\n",
    "        \n",
    "    def tune_params(self):\n",
    "        \"\"\"Function to handle tuning hyperparameters of the model\"\"\"\n",
    "        \n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "        \n",
    "        self.tune_params_called = True\n",
    "        \n",
    "        self.tune_max_features = True\n",
    "        self.tune_n_estimators = True\n",
    "        self.tune_min_samples_leaf = True\n",
    "        \n",
    "        # TODO:\n",
    "        #   - VERIFY THAT THIS IS WORKING\n",
    "        #   - FIND GOOD VALUES OF PARAMETERS TO TEST\n",
    "        \n",
    "        # parameters to tune for RandomForest\n",
    "        \n",
    "        # the maximum number of features to be considered for a tree.\n",
    "        max_features_pos = [None, 'sqrt', 0.2, 0.33, 0.5]\n",
    "        \n",
    "        # number of estimator trees to be built.\n",
    "        # generally the more the better, but takes more\n",
    "        # cpu time\n",
    "        n_estimators_pos = [50, 100, 500, 1000]\n",
    "        \n",
    "        # minimum number of samples in a leaf\n",
    "        min_samples_leaf_pos = [1, 10, 20, 50]\n",
    "        \n",
    "        # stores the parameters along with scores from training\n",
    "        parameters = []\n",
    "        scores = []\n",
    "        \n",
    "        cv_X = self.cv_X_df.values\n",
    "        cv_y = np.ravel(self.cv_y_df.values)\n",
    "        \n",
    "        self.max_features = max_features_pos[int(len(max_features_pos)/2)]\n",
    "        self.n_estimators = n_estimators_pos[int(len(n_estimators_pos)/2)]\n",
    "        self.min_samples_leaf = min_samples_leaf_pos[int(len(min_samples_leaf_pos)/2)]\n",
    "        \n",
    "        if self.regressor:\n",
    "            model = RandomForestRegressor(oob_score=True,\n",
    "                                         n_jobs=self.n_jobs,\n",
    "                                         random_state=1,\n",
    "                                         max_features=self.max_features,\n",
    "                                         n_estimators=self.n_estimators,\n",
    "                                         min_samples_leaf=self.min_samples_leaf)\n",
    "        else:\n",
    "            model = RandomForestClassifier(oob_score=True,\n",
    "                                          n_jobs=self.n_jobs,\n",
    "                                          random_state=1,\n",
    "                                          max_features=self.max_features,\n",
    "                                          n_estimators=self.n_estimators,\n",
    "                                          min_samples_leaf=self.min_samples_leaf)\n",
    "        \n",
    "        # perform feature-space searching\n",
    "        if self.tune_max_features:\n",
    "            scores_max_features = []\n",
    "            for max_features in max_features_pos:\n",
    "                model.set_params(max_features=max_features)\n",
    "                print(model.get_params)\n",
    "                model.fit(cv_X, cv_y)\n",
    "                scores_max_features.append(model.oob_score_)\n",
    "                print(model.oob_score_)\n",
    "            \n",
    "            best_param_index = np.argmax(scores_max_features)\n",
    "            print(best_param_index)\n",
    "            self.max_features = max_features_pos[best_param_index]\n",
    "            model.set_params(max_features=self.max_features)\n",
    "                \n",
    "        if self.tune_n_estimators:\n",
    "            scores_n_estimators = []\n",
    "            for n_estimators in n_estimators_pos:\n",
    "                model.set_params(n_estimators=n_estimators)\n",
    "                print(model.get_params)\n",
    "                model.fit(cv_X, cv_y)\n",
    "                scores_n_estimators.append(model.oob_score_)\n",
    "                print(model.oob_score_)\n",
    "            \n",
    "            best_param_index = np.argmax(scores_n_estimators)\n",
    "            print(best_param_index)\n",
    "            self.n_estimators = n_estimators_pos[best_param_index]\n",
    "            model.set_params(n_estimators=self.n_estimators)\n",
    "                    \n",
    "        if self.tune_min_samples_leaf:\n",
    "            scores_min_samples_leaf = []\n",
    "            for min_samples_leaf in min_samples_leaf_pos:\n",
    "                model.set_params(min_samples_leaf=min_samples_leaf)\n",
    "                print(model.get_params)\n",
    "                model.fit(cv_X, cv_y)\n",
    "                scores_min_samples_leaf.append(model.oob_score_)\n",
    "                print(model.oob_score_)\n",
    "            \n",
    "            best_param_index = np.argmax(scores_min_samples_leaf)\n",
    "            print(best_param_index)\n",
    "            self.min_samples_leaf = min_samples_leaf_pos[best_param_index]\n",
    "            model.set_params(min_samples_leaf=self.min_samples_leaf)\n",
    "        \n",
    "        self.model = model\n",
    "        print(self.model.get_params())\n",
    "        \n",
    "    def train_model(self):\n",
    "        \"\"\"Trains the model on all training data\n",
    "        and returns a model to be used for prediction\"\"\"\n",
    "        \n",
    "        if not(self.read_data_called):\n",
    "            raise AssertionError(\"No data yet!\")\n",
    "            \n",
    "        self.train_model_called = True\n",
    "            \n",
    "        # determine which model to use\n",
    "        if self.tune_params_called:\n",
    "            self.model = self.model\n",
    "        else:\n",
    "            if self.regressor:\n",
    "                model = RandomForestRegressor(oob_score=True,\n",
    "                                             n_jobs=self.n_jobs,\n",
    "                                             random_state=1)\n",
    "            else:\n",
    "                model = RandomForestClassifier(oob_score=True,\n",
    "                                              n_jobs=self.n_jobs,\n",
    "                                              random_state=1)\n",
    "        \n",
    "        if self.select_features_called:\n",
    "            train_X = self.selected_features\n",
    "        else:\n",
    "            train_X = self.train_X_df.values\n",
    "            \n",
    "        train_y = np.ravel(self.train_y_df.values)\n",
    "        \n",
    "        self.model.fit(train_X, train_y)\n",
    "        print(self.model.oob_score_)\n",
    "        \n",
    "    def predict_output(self):\n",
    "        \"\"\"Make predictions on test data.\"\"\"\n",
    "        \n",
    "        # transform the features to match the selected features from training\n",
    "        test_X_all = self.test_X_df.values\n",
    "        test_X = test_X_all[:, self.selected_feature_indices[:1]]\n",
    "        for feature in self.selected_feature_indices[1:]:\n",
    "            \n",
    "            # format data so that it can be added easily\n",
    "            added_feat = np.transpose(test_X_all[:, feature])\n",
    "            added_feat.shape += (1, )\n",
    "            \n",
    "            # append data to the next column\n",
    "            test_X = np.append(test_X, added_feat, axis=1)\n",
    "        \n",
    "        pred = self.model.predict(test_X)\n",
    "        return pred\n",
    "        \n",
    "    def get_model(self):\n",
    "        \"\"\"Return the model used for prediction\"\"\"\n",
    "        return self.model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 2 0 3 4 6]\n",
      "[1 5 2]\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=500, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.802469135802\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=500, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.801346801347\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=500, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.822671156004\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.33, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=500, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.801346801347\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=500, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.813692480359\n",
      "2\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.826038159371\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.822671156004\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=500, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.822671156004\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1000, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.818181818182\n",
      "0\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.793490460157\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=10,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.814814814815\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.826038159371\n",
      "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.2, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=50,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=1, oob_score=True, random_state=1,\n",
      "            verbose=0, warm_start=False)>\n",
      "0.777777777778\n",
      "2\n",
      "{'min_impurity_split': 1e-07, 'criterion': 'gini', 'class_weight': None, 'oob_score': True, 'bootstrap': True, 'random_state': 1, 'min_samples_leaf': 20, 'min_samples_split': 2, 'n_jobs': 1, 'min_weight_fraction_leaf': 0.0, 'max_leaf_nodes': None, 'max_depth': None, 'n_estimators': 50, 'max_features': 0.2, 'warm_start': False, 'verbose': 0}\n",
      "0.784511784512\n",
      "[0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
      " 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1\n",
      " 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1\n",
      " 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1\n",
      " 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForest()\n",
    "\n",
    "model.read_data()\n",
    "\n",
    "imp = model.get_feature_importances()\n",
    "print(imp)\n",
    "\n",
    "features = model.select_features()\n",
    "print(features)\n",
    "\n",
    "model.tune_params()\n",
    "\n",
    "model.train_model()\n",
    "\n",
    "pred = model.predict_output()\n",
    "print(pred)\n",
    "\n",
    "train_model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
