{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some global variables\n",
    "\n",
    "data_filepath = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_int(x):\n",
    "    try: \n",
    "        int(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NONE(X):\n",
    "    \"\"\"Return the values - placeholder function for other operations\"\"\"\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding Functions\n",
    "\n",
    "def MAP(X):\n",
    "    \"\"\"Map all values to integer numbers.\"\"\"\n",
    "    \"\"\"NaN values are treated as a unique value.\"\"\"\n",
    "    \n",
    "    # create an encoding for categorical vars\n",
    "    unique_elems = set(X)\n",
    "    mapping = {label:idx for idx, label in enumerate(unique_elems)}\n",
    "    return X.map(mapping).astype(int)\n",
    "\n",
    "def LOO(X):\n",
    "    \"\"\"Perform Leave One Out counting for the features.\"\"\"\n",
    "    \n",
    "    # map features to ordinal values first\n",
    "    X = MAP(X)\n",
    "    \n",
    "    # perform counts\n",
    "    mapping = {idx:(count-1) for idx, count in enumerate(np.bincount(X))}\n",
    "    return X.map(mapping).astype(int)\n",
    "    \n",
    "\n",
    "def OHE(df, col, idx):\n",
    "    \"\"\"Map categorical values to a one hot encoding scheme.\"\"\"\n",
    "    \n",
    "    # NEEDS TO BE IMPLEMENTED\n",
    "    \n",
    "    # remove the selected column from the df\n",
    "    \n",
    "    \n",
    "    # perform one-hot encoding\n",
    "    \n",
    "    \n",
    "    # create new column names of form oldcolname-1, oldcolname-2, ...\n",
    "    \n",
    "    \n",
    "    # add the cols back to the df\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling Functions\n",
    "\n",
    "def NRM1(X):\n",
    "    \"\"\"Scale by dividing by the 1-norm\"\"\"\n",
    "    norm = np.linalg.norm(X, ord=1)\n",
    "    return X / norm\n",
    "\n",
    "def SCL1(X):\n",
    "    \"\"\"Scale between (-1, 1)\"\"\"\n",
    "    mean = X.mean()\n",
    "    maximum = X.max()\n",
    "    minimum = X.min()\n",
    "    return (X - mean) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imputing Functions\n",
    "\n",
    "def UNIQ(X, value=-1):\n",
    "    \"\"\"Replace missing Values with unique value\"\"\"\n",
    "    \n",
    "    X.fillna(value=value, inplace=True)    \n",
    "    return X\n",
    "\n",
    "def MEAN(X):\n",
    "    \"\"\"Replace missing values with the mean of the others\"\"\"\n",
    "    \n",
    "    mean = np.mean(X)\n",
    "    X.fillna(value=mean, inplace=True)\n",
    "    return X\n",
    "\n",
    "def MED(X):\n",
    "    \"\"\"Replace missing values with median of data\"\"\"\n",
    "    \n",
    "    median = np.nanmedian(X)\n",
    "    X.fillna(value=median, inplace=True)\n",
    "    return X\n",
    "\n",
    "def CONST(X, value=0):\n",
    "    \"\"\"Replace missing values with a constant.\"\"\"\n",
    "    \n",
    "    X.fillna(value=value, inplace=True)\n",
    "    return X\n",
    "\n",
    "def MODE(X):\n",
    "    \"\"\"Replace missing values with the mode.\"\"\"\n",
    "    \n",
    "    mode = stats.mode(X)[0][0]\n",
    "    X.fillna(value=mode, inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, train_data_file, train_label_file, train_ids_file,\n",
    "                 instr_file, test_data_file=None, test_ids_file=None):\n",
    "        \"\"\"A class to process and reformat data\n",
    "        for use in learning models\"\"\"\n",
    "        \n",
    "        # initialize the data the data filenames\n",
    "        self.train_data_file = train_data_file\n",
    "        self.train_label_file = train_label_file\n",
    "        self.train_ids_file = train_ids_file\n",
    "        self.instr_file = instr_file\n",
    "        \n",
    "        # test data is optional\n",
    "        self.test_data_file = test_data_file\n",
    "        self.test_ids_file = test_ids_file\n",
    "        \n",
    "    def read_data(self):\n",
    "        \"\"\"Reads in data from the files passed to constructor\"\"\"\n",
    "        \n",
    "        # read in the data\n",
    "        train_X_df = pd.read_csv(self.train_data_file)\n",
    "        train_y_df = pd.read_csv(self.train_label_file)\n",
    "        train_ids_df = pd.read_csv(self.train_ids_file)\n",
    "        self.instr_df = pd.read_csv(self.instr_file)\n",
    "        \n",
    "        self.feature_names = [feature for feature in train_X_df]\n",
    "        self.label_names = [feature for feature in train_y_df]\n",
    "        self.id_names = [feature for feature in train_ids_df]\n",
    "        \n",
    "        # create cross validation data\n",
    "        self.cv_X_df = pd.DataFrame(train_X_df)\n",
    "        self.cv_y_df = pd.DataFrame(train_y_df)\n",
    "        self.cv_ids_df = pd.DataFrame(train_ids_df)\n",
    "        \n",
    "        # read in the test data if it exists\n",
    "        if self.test_data_file != None:\n",
    "            self.test_X_df = pd.read_csv(self.test_data_file)\n",
    "            self.test_ids_df = pd.read_csv(self.test_ids_file)\n",
    "            self.all_X_df = train_X_df.append(self.test_X_df)\n",
    "        else:\n",
    "            self.test_X_df = None\n",
    "            self.test_ids_df = None\n",
    "            self.all_X_df = pd.DataFrame(train_X_df)\n",
    "        \n",
    "        # determine the shape of the input data\n",
    "        self.train_X_shape = train_X_df.shape\n",
    "        self.train_y_shape = train_y_df.shape\n",
    "        self.train_ids_shape = train_ids_df.shape\n",
    "        self.instr_shape = self.instr_df.shape\n",
    "        self.all_shape = self.all_X_df.shape\n",
    "        \n",
    "        # get size of test data if it exists\n",
    "        if self.test_data_file != None:\n",
    "            self.test_X_shape = self.test_X_df.shape\n",
    "            self.test_ids_shape = self.test_ids_df.shape\n",
    "        else:\n",
    "            self.test_X_shape = None\n",
    "            self.test_ids_shape = None\n",
    "\n",
    "        \n",
    "    def process(self):\n",
    "        \"\"\"Performs the processing on cross validation and train/test data\"\"\"\n",
    "        \n",
    "        # processing on all data - remember to include cv_X and all_X for each condition\n",
    "        for idx, col in enumerate(self.feature_names):\n",
    "            \n",
    "            # determine what to perform at each of the steps\n",
    "            col_instr = self.instr_df[col].values\n",
    "            col_enc = col_instr[1]\n",
    "            col_scl = col_instr[2]\n",
    "            col_imp = col_instr[3]\n",
    "\n",
    "            # impute values\n",
    "            # imputed first so that other functions will not use nan values in calculations\n",
    "            if col_imp == 'UNIQ':\n",
    "                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n",
    "                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n",
    "            if col_imp == 'MEAN':\n",
    "                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = MEAN(self.all_X_df[col])\n",
    "            if col_imp == 'MODE':\n",
    "                self.cv_X_df[col] = MODE(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = MODE(self.all_X_df[col])\n",
    "            if col_imp == 'MED':\n",
    "                self.cv_X_df[col] = MED(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = MED(self.all_X_df[col])\n",
    "            if is_int(col_imp):\n",
    "                self.cv_X_df[col] = CONST(self.cv_X_df[col], col_imp)\n",
    "                self.all_X_df[col] = CONST(self.all_X_df[col], col_imp)\n",
    "            \n",
    "            \n",
    "            # perform encoding of data\n",
    "            if col_enc == 'MAP':\n",
    "                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = MAP(self.all_X_df[col])\n",
    "            if col_enc == 'OHE':\n",
    "                self.cv_X_df = OHE(self.cv_X_df, col, idx)\n",
    "                self.all_X_df = OHE(self.all_X_df, col, idx)\n",
    "            if col_enc == 'LOO':\n",
    "                self.cv_X_df[col] = LOO(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = LOO(self.all_X_df[col])\n",
    "            \n",
    "\n",
    "            # perform scaling\n",
    "            if col_scl == 'NRM1':\n",
    "                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = NRM1(self.all_X_df[col])\n",
    "            if col_scl == 'SCL1':\n",
    "                self.cv_X_df[col] = SCL1(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = SCL1(self.all_X_df[col])\n",
    "\n",
    "        \n",
    "        # get the values from the dataframes\n",
    "        self.cv_X = self.cv_X_df.values\n",
    "        self.cv_y = self.cv_y_df.values\n",
    "        self.cv_ids = self.cv_ids_df.values\n",
    "        \n",
    "        all_X = self.all_X_df.values\n",
    "        self.train_X = all_X[:self.train_X_shape[0], :]\n",
    "        self.train_y = self.cv_y_df.values\n",
    "        self.train_ids = self.cv_ids_df.values\n",
    "        \n",
    "        if self.test_data_file != None:\n",
    "            self.test_X = all_X[self.train_X_shape[0]:, :]\n",
    "            self.test_ids = self.test_ids_df.values\n",
    "        else:\n",
    "            self.test_X = None\n",
    "            self.test_ids = None\n",
    "        \n",
    "    def write_data(self, out_dir='./processed_data/'):\n",
    "        \"\"\"Writes all of the data to output files\"\"\"\n",
    "        \n",
    "        # create the output directory if it does not exist\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            \n",
    "        # convert arrays back into DataFrames\n",
    "        cv_X_df = pd.DataFrame(self.cv_X,  columns=self.feature_names)\n",
    "        cv_y_df = pd.DataFrame(self.cv_y, columns=self.label_names)\n",
    "        cv_ids_df = pd.DataFrame(self.cv_ids, columns=self.id_names)\n",
    "        train_X_df = pd.DataFrame(self.train_X, columns=self.feature_names)\n",
    "        train_y_df = pd.DataFrame(self.train_y, columns=self.label_names)\n",
    "        train_ids_df = pd.DataFrame(self.train_ids, columns=self.id_names)\n",
    "        if self.test_data_file != None:\n",
    "            test_X_df = pd.DataFrame(self.test_X, columns=self.feature_names)\n",
    "            test_ids_df = pd.DataFrame(self.test_ids, columns=self.id_names)\n",
    "        \n",
    "        # write the dataframes to file\n",
    "        cv_X_df.to_csv(out_dir+'cv_X.csv', index=False)\n",
    "        cv_y_df.to_csv(out_dir+'cv_y.csv', index=False)\n",
    "        cv_ids_df.to_csv(out_dir+'cv_ids.csv', index=False)\n",
    "        train_X_df.to_csv(out_dir+'train_X.csv', index=False)\n",
    "        train_y_df.to_csv(out_dir+'train_y.csv', index=False)\n",
    "        train_ids_df.to_csv(out_dir+'train_ids.csv', index=False)\n",
    "        if self.test_data_file != None:\n",
    "            test_X_df.to_csv(out_dir+'test_X.csv', index=False)\n",
    "            test_ids_df.to_csv(out_dir+'test_ids.csv', index=False)\n",
    "        \n",
    "    def select_features(self):\n",
    "        \"\"\"Perform features selection / compression algs like PCA.\"\"\"\n",
    "        \"\"\"These will be implemented once more has been done.\"\"\"\n",
    "        self.feature_names = self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some simple testing code and such\n",
    "train_data = data_filepath+'houseprices_data_train.csv'\n",
    "train_labels = data_filepath+'houseprices_labels_train.csv'\n",
    "train_ids = data_filepath+'houseprices_ids_train.csv'\n",
    "test_data = data_filepath+'houseprices_data_test.csv'\n",
    "test_ids = data_filepath+'houseprices_ids_test.csv'\n",
    "description = data_filepath+'houseprices_feature_descriptions.csv'\n",
    "\n",
    "proc = Preprocessor(train_data_file=train_data,\n",
    "                 train_label_file=train_labels,\n",
    "                 train_ids_file=train_ids,\n",
    "                 test_data_file=test_data,\n",
    "                 test_ids_file=test_ids,\n",
    "                 instr_file=description)\n",
    "\n",
    "proc.read_data()\n",
    "\n",
    "proc.process()\n",
    "\n",
    "# doesn't do anything yet, hasn't been implemented\n",
    "proc.select_features()\n",
    "\n",
    "# data is written to output directory\n",
    "# any existing data is overwritten\n",
    "proc.write_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
