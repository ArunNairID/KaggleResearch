{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some global variables\n",
    "\n",
    "data_filepath = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NONE(X):\n",
    "    \"\"\"Return the values - placeholder function for other operations\"\"\"\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding Functions\n",
    "\n",
    "def MAP(X):\n",
    "    \"\"\"Map all values to integer numbers.\"\"\"\n",
    "    \"\"\"NaN values are treated as a unique value.\"\"\"\n",
    "    \n",
    "    # create an encoding for categorical vars\n",
    "    unique_elems = set(X)\n",
    "    mapping = {label:idx for idx, label in enumerate(unique_elems)}\n",
    "    return X.map(mapping).astype(int)\n",
    "\n",
    "def OHE(df, col):\n",
    "    \"\"\"Map categorical values to a one hot encoding scheme.\"\"\"\n",
    "    \n",
    "    # NEEDS TO BE IMPLEMENTED\n",
    "    \n",
    "    # remove the selected column from the df\n",
    "    \n",
    "    \n",
    "    # perform one-hot encoding\n",
    "    \n",
    "    \n",
    "    # create new column names of form oldcolname-1, oldcolname-2, ...\n",
    "    \n",
    "    \n",
    "    # add the cols back to the df\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling Functions\n",
    "\n",
    "def NRM1(X):\n",
    "    \"\"\"Scale by dividing by the 1-norm\"\"\"\n",
    "    norm = np.linalg.norm(X, ord=1)\n",
    "    return X / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imputing Functions\n",
    "\n",
    "def UNIQ(X, value=-1):\n",
    "    \"\"\"Replace missing Values with unique value\"\"\"\n",
    "    \n",
    "    X.fillna(value=value, inplace=True)    \n",
    "    return X\n",
    "\n",
    "def MEAN(X):\n",
    "    \"\"\"Replace missing values with the mean of the others\"\"\"\n",
    "    \n",
    "    mean = np.mean(X)\n",
    "    X.fillna(value=mean, inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, train_data_file, train_label_file, train_ids_file,\n",
    "                 instr_file, test_data_file=None, test_ids_file=None):\n",
    "        \"\"\"A class to process and reformat data\n",
    "        for use in learning models\"\"\"\n",
    "        \n",
    "        # initialize the data the data filenames\n",
    "        self.train_data_file = train_data_file\n",
    "        self.train_label_file = train_label_file\n",
    "        self.train_ids_file = train_ids_file\n",
    "        self.instr_file = instr_file\n",
    "        \n",
    "        # test data is optional\n",
    "        self.test_data_file = test_data_file\n",
    "        self.test_ids_file = test_ids_file\n",
    "        \n",
    "        # initialize this to features names from the train data\n",
    "        self.features = []\n",
    "        \n",
    "    def read_data(self):\n",
    "        \"\"\"Reads in data from the files passed to constructor\"\"\"\n",
    "        \n",
    "        # read in the data\n",
    "        train_X_df = pd.read_csv(self.train_data_file)\n",
    "        train_y_df = pd.read_csv(self.train_label_file)\n",
    "        train_ids_df = pd.read_csv(self.train_ids_file)\n",
    "        self.instr_df = pd.read_csv(self.instr_file)\n",
    "        \n",
    "        # create cross validation data\n",
    "        self.cv_X_df = pd.DataFrame(train_X_df)\n",
    "        self.cv_y_df = pd.DataFrame(train_y_df)\n",
    "        self.cv_ids_df = pd.DataFrame(train_ids_df)\n",
    "        \n",
    "        # read in the test data if it exists\n",
    "        if self.test_data_file != None:\n",
    "            self.test_X_df = pd.read_csv(self.test_data_file)\n",
    "            self.test_ids_df = pd.read_csv(self.test_ids_file)\n",
    "            self.all_X_df = train_X_df.append(self.test_X_df)\n",
    "        else:\n",
    "            self.test_X_df = None\n",
    "            self.test_ids_df = None\n",
    "            self.all_X_df = pd.DataFrame(train_X_df)\n",
    "        \n",
    "        # determine the shape of the input data\n",
    "        self.train_X_shape = train_X_df.shape\n",
    "        self.train_y_shape = train_y_df.shape\n",
    "        self.train_ids_shape = train_ids_df.shape\n",
    "        self.instr_shape = self.instr_df.shape\n",
    "        self.all_shape = self.all_X_df.shape\n",
    "        \n",
    "        # get size of test data if it exists\n",
    "        if self.test_data_file != None:\n",
    "            self.test_X_shape = self.test_X_df.shape\n",
    "            self.test_ids_shape = self.test_ids_df.shape\n",
    "        else:\n",
    "            self.test_X_shape = None\n",
    "            self.test_ids_shape = None\n",
    "\n",
    "        \n",
    "    def process(self):\n",
    "        \"\"\"Performs the processing on cross validation and train/test data\"\"\"\n",
    "        \n",
    "        # processing on all data - remember to include cv_X and all_X for each condition\n",
    "        for col in self.cv_X_df:\n",
    "            \n",
    "            # determine what to perform at each of the steps\n",
    "            col_instr = self.instr_df[col].values\n",
    "            col_enc = col_instr[1]\n",
    "            col_scl = col_instr[2]\n",
    "            col_imp = col_instr[3]\n",
    "\n",
    "            # impute values\n",
    "            # imputed first so that other functions will not use nan values in calculations\n",
    "            if col_imp == 'UNIQ':\n",
    "                self.cv_X_df[col] = UNIQ(self.cv_X_df[col], value=-1)\n",
    "                self.all_X_df[col] = UNIQ(self.all_X_df[col], value=-1)\n",
    "            if col_imp == 'MEAN':\n",
    "                self.cv_X_df[col] = MEAN(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = MEAN(self.all_X_df[col])\n",
    "            \n",
    "            \n",
    "            # perform encoding of data\n",
    "            if col_enc == 'MAP':\n",
    "                self.cv_X_df[col] = MAP(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = MAP(self.cv_X_df[col])\n",
    "            if col_enc == 'OHE':\n",
    "                self.cv_X_df = OHE(self.cv_X_df, col)\n",
    "                self.all_X_df = OHE(self.all_X_df, col)\n",
    "            \n",
    "\n",
    "            # perform scaling\n",
    "            if col_scl == 'NRM1':\n",
    "                self.cv_X_df[col] = NRM1(self.cv_X_df[col])\n",
    "                self.all_X_df[col] = NRM1(self.all_X_df[col])\n",
    "\n",
    "        \n",
    "        # get the values from the dataframes\n",
    "        self.cv_X = self.cv_X_df.values\n",
    "        self.cv_y = self.cv_y_df.values\n",
    "        self.cv_ids = self.cv_ids_df.values\n",
    "        \n",
    "        all_X = self.all_X_df.values\n",
    "        self.train_X = all_X[:self.train_X_shape[0], :]\n",
    "        self.train_y = self.cv_y_df.values\n",
    "        self.train_ids = self.cv_ids_df.values\n",
    "        \n",
    "        if self.test_data_file != None:\n",
    "            self.test_X = all_X[self.train_X_shape[0]:, :]\n",
    "            self.test_ids = self.test_ids_df.values\n",
    "        else:\n",
    "            self.test_X = None\n",
    "            self.test_ids = None\n",
    "\n",
    "        \n",
    "    def write_data(self, out_dir='./processed_data/'):\n",
    "        \"\"\"Writes all of the data to output files\"\"\"\n",
    "        \n",
    "        # create the output directory if it does not exist\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            \n",
    "        # TODO: \n",
    "        #   - CREATE A WAY TO ADD BACK THE FEATURE NAMES\n",
    "            \n",
    "        # convert arrays back into DataFrames\n",
    "        cv_X_df = pd.DataFrame(self.cv_X)\n",
    "        cv_y_df = pd.DataFrame(self.cv_y)\n",
    "        cv_ids_df = pd.DataFrame(self.cv_ids)\n",
    "        train_X_df = pd.DataFrame(self.train_X)\n",
    "        train_y_df = pd.DataFrame(self.train_y)\n",
    "        train_ids_df = pd.DataFrame(self.train_ids)\n",
    "        if self.test_data_file != None:\n",
    "            test_X_df = pd.DataFrame(self.test_X)\n",
    "            test_ids_df = pd.DataFrame(self.test_ids)\n",
    "        \n",
    "        # write the dataframes to file\n",
    "        cv_X_df.to_csv(out_dir+'cv_X.csv')\n",
    "        cv_y_df.to_csv(out_dir+'cv_y.csv')\n",
    "        cv_ids_df.to_csv(out_dir+'cv_ids.csv')\n",
    "        train_X_df.to_csv(out_dir+'train_X.csv')\n",
    "        train_y_df.to_csv(out_dir+'train_y.csv')\n",
    "        train_ids_df.to_csv(out_dir+'train_ids.csv')\n",
    "        if self.test_data_file != None:\n",
    "            test_X_df.to_csv(out_dir+'test_X.csv')\n",
    "            test_ids_df.to_csv(out_dir+'test_ids.csv')\n",
    "        \n",
    "        \n",
    "    def select_features(self):\n",
    "        \"\"\"Perform features selection / compression algs like PCA.\"\"\"\n",
    "        \"\"\"These will be implemented once more has been done.\"\"\"\n",
    "        self.features = self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some simple testing code and such\n",
    "train_data = data_filepath+'example_data_train.csv'\n",
    "train_labels = data_filepath+'example_labels_train.csv'\n",
    "train_ids = data_filepath+'example_ids_train.csv'\n",
    "test_data = data_filepath+'example_data_test.csv'\n",
    "test_ids = data_filepath+'example_ids_test.csv'\n",
    "description = data_filepath+'FeatureDescriptions.csv'\n",
    "\n",
    "proc = Preprocessor(train_data_file=train_data,\n",
    "                 train_label_file=train_labels,\n",
    "                 train_ids_file=train_ids,\n",
    "                 test_data_file=test_data,\n",
    "                 test_ids_file=test_ids,\n",
    "                 instr_file=description)\n",
    "\n",
    "proc.read_data()\n",
    "\n",
    "proc.process()\n",
    "\n",
    "# doesn't do anything yet, hasn't been implemented\n",
    "proc.select_features()\n",
    "\n",
    "# data is written to output directory\n",
    "# any existing data is overwritten\n",
    "proc.write_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
