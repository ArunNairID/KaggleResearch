{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data...\n",
      "    test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
      "0     3036.694434     24.421653     3036.694434       2.713561\n",
      "1     3036.559253     24.421653     3036.559228       2.713542\n",
      "2     3036.394189     24.421687     3036.394189       2.713542\n",
      "3     3036.192505     24.421681     3036.192529       2.713542\n",
      "4     3035.946387     24.421653     3035.946313       2.713488\n",
      "5     3035.645679     24.421653     3035.645630       2.713474\n",
      "6     3035.278418     24.421688     3035.278467       2.713542\n",
      "7     3034.830054     24.421671     3034.830029       2.713535\n",
      "8     3034.282568     24.421641     3034.282568       2.713511\n",
      "9     3033.614087     24.421684     3033.614136       2.713521\n",
      "10    3032.798096     24.421684     3032.797998       2.713542\n",
      "11    3031.801880     24.421714     3031.801880       2.713521\n",
      "12    3030.585889     24.421738     3030.585864       2.713502\n",
      "13    3029.102002     24.421735     3029.101929       2.713532\n",
      "14    3027.291358     24.421716     3027.291333       2.713469\n",
      "15    3025.082642     24.421814     3025.082617       2.713499\n",
      "16    3022.388965     24.421906     3022.388916       2.713529\n",
      "17    3019.104858     24.422126     3019.104858       2.713553\n",
      "18    3015.102759     24.422410     3015.102759       2.713540\n",
      "19    3010.227978     24.422714     3010.227979       2.713520\n",
      "20    3004.293774     24.423151     3004.293774       2.713478\n",
      "21    2997.075244     24.423777     2997.075171       2.713442\n",
      "22    2988.302222     24.424409     2988.302100       2.713377\n",
      "23    2977.651343     24.425168     2977.651245       2.713314\n",
      "24    2964.738550     24.425804     2964.738501       2.712938\n",
      "25    2949.109570     24.426683     2949.109424       2.712591\n",
      "26    2930.229663     24.428627     2930.229468       2.712046\n",
      "27    2907.477417     24.432277     2907.477026       2.711325\n",
      "28    2880.138037     24.437279     2880.137622       2.710339\n",
      "29    2847.403198     24.446784     2847.402734       2.708940\n",
      "..            ...           ...             ...            ...\n",
      "70    1929.351196     14.965947     1924.278247       2.638055\n",
      "71    1929.734997     14.923285     1924.493005       2.674251\n",
      "72    1930.048730     14.913851     1924.639002       2.731976\n",
      "73    1930.272253     14.851045     1924.713355       2.715895\n",
      "74    1930.351404     14.857743     1924.663208       2.697208\n",
      "75    1930.460449     14.843353     1924.678918       2.654512\n",
      "76    1930.576196     14.796816     1924.636926       2.686493\n",
      "77    1930.748889     14.781547     1924.549206       2.713139\n",
      "78    1930.687830     14.811206     1924.374719       2.675128\n",
      "79    1930.782312     14.841107     1924.243750       2.656905\n",
      "80    1930.718799     14.950882     1924.032556       2.630358\n",
      "81    1930.695557     14.946719     1923.915039       2.619932\n",
      "82    1930.705322     14.988774     1923.749597       2.594366\n",
      "83    1930.686060     14.969752     1923.602332       2.605964\n",
      "84    1930.688647     14.960078     1923.475549       2.602768\n",
      "85    1930.660620     14.964426     1923.309973       2.585854\n",
      "86    1930.629614     14.935528     1923.132068       2.587446\n",
      "87    1930.618408     14.927208     1922.986938       2.593035\n",
      "88    1930.589123     14.951863     1922.861560       2.576828\n",
      "89    1930.550378     14.961940     1922.729566       2.536617\n",
      "90    1930.534888     14.985487     1922.607922       2.549161\n",
      "91    1930.477966     15.057896     1922.422107       2.505845\n",
      "92    1930.391357     15.046159     1922.215173       2.527615\n",
      "93    1930.369165     15.020885     1922.078626       2.525095\n",
      "94    1930.327722     15.032203     1921.936841       2.507006\n",
      "95    1930.252222     15.002199     1921.780603       2.550972\n",
      "96    1930.255701     15.037891     1921.616614       2.554536\n",
      "97    1930.243481     15.069281     1921.509119       2.526727\n",
      "98    1930.238037     15.051958     1921.401037       2.530871\n",
      "99    1930.151831     15.033507     1921.175391       2.559607\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "[ 2791.28955078  2750.70458984  2721.91650391 ...,  3416.60327148\n",
      "  2971.46801758  2156.06713867]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "def write_hyperparams(hyperparams, fileName):\n",
    "    f = open(fileName, 'w')\n",
    "    f.write(' '.join(str(x) for x in hyperparams) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def read_hyperparams(fileName):\n",
    "    f = open(fileName, 'r')\n",
    "    n_folds, n_estimators, max_depth, max_features, \\\n",
    "        score = f.read().split()\n",
    "    return int(n_folds), int(n_estimators), int(max_depth), \\\n",
    "        max_features, float(score)\n",
    "\n",
    "# define some constants\n",
    "xgboost_data_dir = '../xgboost_data/'\n",
    "data_dir = '../data/'\n",
    "hyperParamFile = 'hyperparams_cat.txt'\n",
    "n_folds = 3\n",
    "verbose_tree = 3\n",
    "verbose_grid = 0\n",
    "n_jobs = 3\n",
    "random_state = 1\n",
    "criterion = 'mae'\n",
    "\n",
    "# For xgboost\n",
    "param = {}\n",
    "param['objective'] = 'reg:gamma'\n",
    "param['nthread'] = 2\n",
    "param['eval_metric'] = 'mae'\n",
    "param['max_depth'] = 4\n",
    "param['eta'] = 0.2\n",
    "param['silent'] = 1  # prints messages to screen (1 silences these)\n",
    "param['tree_method'] = 'auto'\n",
    "param['lambda'] = 0.1\n",
    "num_boost_rounds = 100\n",
    "\n",
    "# allows for validation to occur during training\n",
    "evallist = []\n",
    "\n",
    "# read in the train dataset\n",
    "print('loading training data...')\n",
    "# for xgboost\n",
    "dtrain = xgb.DMatrix(xgboost_data_dir+'continuous_selected.dat')\n",
    "\n",
    "evaluations = xgb.cv(params=param, dtrain=dtrain, num_boost_round=num_boost_rounds,\n",
    "            nfold=10)\n",
    "print(evaluations)\n",
    "\n",
    "\n",
    "tree = xgb.train( param, dtrain, num_boost_rounds, evallist)\n",
    "tree.save_model('prac.model')\n",
    "\n",
    "dtest = xgb.DMatrix(xgboost_data_dir+'continuous_test_selected.dat')\n",
    "y_pred = tree.predict(dtest)\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "def write_hyperparams(hyperparams, fileName):\n",
    "\tf = open(fileName, 'w')\n",
    "\tf.write(' '.join(str(x) for x in hyperparams) + '\\n')\n",
    "\tf.close()\n",
    "\n",
    "def read_hyperparams(fileName):\n",
    "\tf = open(fileName, 'r')\n",
    "\tn_folds, n_estimators, max_depth, max_features, \\\n",
    "\t\tscore = f.read().split()\n",
    "\treturn int(n_folds), int(n_estimators), int(max_depth), \\\n",
    "\t\tmax_features, float(score)\n",
    "\n",
    "# define some constants\n",
    "hyperParamFile = 'hyperparams_cont.txt'\n",
    "n_folds = 3\n",
    "verbose_tree = 3\n",
    "verbose_grid = 0\n",
    "n_jobs = 4\n",
    "random_state = 1\n",
    "criterion = 'mae'\n",
    "\n",
    "# read in the train dataset\n",
    "print('loading training data...')\n",
    "df_cont = pd.read_csv('continuous_selected.csv', header=0, index_col=0)\n",
    "df_y = pd.read_csv('y_all.csv', header=0, index_col=0)\n",
    "\n",
    "X = df_cont.values\n",
    "y = np.ravel(df_y.values)\n",
    "\n",
    "# create a Random Forest Classifier\n",
    "print('creating a model...')\n",
    "n_estimators_def = 100\n",
    "max_features_def = 'sqrt'\n",
    "max_depth_def = 10\n",
    "#n_estimators_range = [10, 50]\n",
    "#max_features_range = ['sqrt']\n",
    "#max_depth_range = [10, 30]\n",
    "#param_grid = {'n_estimators': n_estimators_range, 'max_features': max_features_range,\n",
    "#\t'max_depth': max_depth_range}\n",
    "\n",
    "# create a tree to train the models on\n",
    "tree = RandomForestRegressor(criterion=criterion, verbose=verbose_tree, n_jobs=n_jobs,\n",
    "\trandom_state=random_state, n_estimators=n_estimators_def,\n",
    "\tmax_features=max_features_def, max_depth=max_depth_def)\n",
    "\n",
    "# some feature selection\n",
    "#print('selecting features...')\n",
    "#print('input feature shape: ')\n",
    "#print(X.shape)\n",
    "#tree.fit(X, y)\n",
    "#feature_select = SelectFromModel(tree, prefit=True, threshold=feature_threshold)\n",
    "##print(tree.feature_importances_.sort())\n",
    "#X_new = feature_select.transform(X)\n",
    "#print('new input feature shape: ')\n",
    "#print(X_new.shape)\n",
    "\n",
    "# perform a grid search to tune the paramteres\n",
    "#print('grid search to tune hyperparameters...')\n",
    "#gs = GridSearchCV(estimator=tree,\n",
    "#\tparam_grid=param_grid, scoring=None,\n",
    "#\tcv=n_folds, n_jobs=n_jobs, verbose=verbose_grid)\n",
    "#gs = gs.fit(X_new, y)\n",
    "#print(gs.scorer_)\n",
    "#print('best score from grid search: %.3f' % gs.best_score_)\n",
    "#print(gs.best_params_)\n",
    "#best = gs.best_params_\n",
    "#n_estimators_gs = best['n_estimators']\n",
    "#max_depth_gs = best['max_depth']\n",
    "#max_features_gs = best['max_features']\n",
    "\n",
    "# run some cross validation\n",
    "print('running cross validation to determine accuracy of model...')\n",
    "scores = []\n",
    "splits = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "for train, test in splits.split(X):\n",
    "\ttree.fit(X[train], y[train])\n",
    "\tpredicted = tree.predict(X[test])\n",
    "\tscore = mean_absolute_error(y[test], predicted)\n",
    "\tscores.append(score)\n",
    "print(scores)\n",
    "\n",
    "# determine which features to write to the file\n",
    "n_estimators = n_estimators_def\n",
    "max_depth = max_depth_def\n",
    "max_features=max_features_def\n",
    "score = np.mean(scores)\n",
    "\n",
    "print('writing the data to file...')\n",
    "params = (n_folds, n_estimators, max_depth, max_features, score)\n",
    "write_hyperparams(params, hyperParamFile)\n",
    "n_folds, n_estimators, max_depth, max_features, \\\n",
    "\tscore = read_hyperparams(hyperParamFile)\n",
    "print(n_folds, n_estimators, max_depth, max_features, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
