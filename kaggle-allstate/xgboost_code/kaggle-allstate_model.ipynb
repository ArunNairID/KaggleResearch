{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# dump data to xgboost files\n",
    "from sklearn.datasets import dump_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some constants\n",
    "\n",
    "n_jobs = 2\n",
    "random_state = 1\n",
    "\n",
    "# data\n",
    "data_dir = '../data/'\n",
    "xgboost_data_dir = '../xgboost_data/'\n",
    "hyperParamFile = 'hyperparams_cat.txt'\n",
    "\n",
    "# size of data\n",
    "train_size = 188318\n",
    "test_size = 125546\n",
    "\n",
    "# features to be selected\n",
    "num_features_cat = 50\n",
    "num_features_cont = 14\n",
    "\n",
    "# training parameters\n",
    "criterion = 'mae'\n",
    "n_folds = 3  # cross validation\n",
    "\n",
    "# random forest params\n",
    "verbose_tree = 0\n",
    "\n",
    "# xgboost params\n",
    "early_stopping_rounds = 20\n",
    "num_boost_rounds = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_hyperparams(hyperparams, fileName):\n",
    "    f = open(fileName, 'w')\n",
    "    f.write(' '.join(str(x) for x in hyperparams) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def read_hyperparams(fileName):\n",
    "    f = open(fileName, 'r')\n",
    "    n_folds, n_estimators, max_depth, max_features, \\\n",
    "        score = f.read().split()\n",
    "    return int(n_folds), int(n_estimators), int(max_depth), \\\n",
    "        max_features, float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to take the data and build features that can then be used to train a model\n",
    "\n",
    "def build_features(df):\n",
    "    ''' build the features to use for the model'''\n",
    "    \n",
    "    # remove the labels and the ids for use\n",
    "    \n",
    "    y = df.pop('loss').values if 'loss' in list(df) else None\n",
    "    ids = df.pop('id').values\n",
    "\n",
    "    # do not always impute data for tree models\n",
    "    # compute using another category\n",
    "    \n",
    "    # create an imputer for imputer values later\n",
    "    imputer = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "    \n",
    "    # determine whether data is categorical or continuous\n",
    "    leave_one_out_cols = []\n",
    "    leave_one_out_counts = []\n",
    "    one_hot_cols = []\n",
    "    col_count = 0\n",
    "    for column in df:\n",
    "\n",
    "        # determine if the column is categorical or not\n",
    "        if 'cat' in column:\n",
    "\n",
    "            # create an encoding for categorical vars\n",
    "            mapping = {label:idx for idx, label in \\\n",
    "                enumerate(np.unique(df[column]))}\n",
    "\n",
    "            # convert everything into integers for categorical\n",
    "            df[column] = df[column].map(mapping)\n",
    "            df[column] = df[column].astype(int)\n",
    "            \n",
    "            unique_elems = len(mapping)\n",
    "\n",
    "            # perform leave-one-out counting\n",
    "            if unique_elems > 10:\n",
    "                #df[column] = [df[column].values.tolist().count(x)-1 for \\\n",
    "                #\tx in df[column].values.tolist()]\n",
    "                leave_one_out_cols.append(col_count)\n",
    "                # initialize counts to -1 for Leave One Out counting\n",
    "                leave_one_out_counts.append([-1 for x in range(unique_elems)])\n",
    "            else:\n",
    "                one_hot_cols.append(col_count)\n",
    "\n",
    "        col_count += 1\n",
    "\n",
    "    imputer = imputer.fit(df)\n",
    "    X = imputer.transform(df.values)\n",
    "    X_cat = X[:, :116]\n",
    "    X_cont = X[:, 116:]\n",
    "    \n",
    "    # transform data to leave-one-out counting\n",
    "    for num, col in enumerate(leave_one_out_cols):\n",
    "        # count the data\n",
    "        for idx, value in enumerate(X_cat[:, col]):\n",
    "            leave_one_out_counts[num][int(value)] = leave_one_out_counts[num][int(value)] + 1\n",
    "        # apply the counted data to form LOO data\n",
    "        for idx, value in enumerate(X_cat[:, col]):\n",
    "            X_cat[idx][col] = leave_one_out_counts[num][int(value)] \n",
    "            \n",
    "    # transform data to one-hot encoded\n",
    "    one_hot_encoder = OneHotEncoder(categorical_features=one_hot_cols, sparse=False)\n",
    "    X_cat = one_hot_encoder.fit_transform(X_cat)\n",
    "    \n",
    "    print('X cat size')\n",
    "    print(X_cat.shape)\n",
    "    print('X cont size')\n",
    "    print(X_cont.shape)\n",
    "    \n",
    "    return X_cat, X_cont, y, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "processing cross validation data...\n",
      "X cat size\n",
      "(188318, 297)\n",
      "X cont size\n",
      "(188318, 14)\n",
      "processing all data...\n",
      "X cat size\n",
      "(313864, 301)\n",
      "X cont size\n",
      "(313864, 14)\n"
     ]
    }
   ],
   "source": [
    "# read in the train dataset\n",
    "print('loading data...')\n",
    "\n",
    "df_train = pd.read_csv(data_dir + 'train.csv', header=0)\n",
    "df_test = pd.read_csv(data_dir + 'test.csv', header=0)\n",
    "df_all = df_train.append(df_test)  # we want to process all data at the same time for counts and such\n",
    "\n",
    "print('processing cross validation data...')\n",
    "# process the training data alone for cross validation\n",
    "X_cv_cat, X_cv_cont, y_cv, ids_cv = build_features(df_train)\n",
    "\n",
    "print('processing all data...')\n",
    "# process the training and test data together\n",
    "X_all_cat, X_all_cont, y_all, ids_all = build_features(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# separate the data into training and test sets\n",
    "\n",
    "# test\n",
    "X_test_cat = X_all_cat[train_size:, :]\n",
    "X_test_cont = X_all_cont[train_size:, :]\n",
    "y_test = y_all[train_size:]\n",
    "ids_test = ids_all[train_size:]\n",
    "\n",
    "# train\n",
    "X_train_cat = X_all_cat[:train_size, :]\n",
    "X_train_cont = X_all_cont[:train_size, :]\n",
    "y_train = y_all[:train_size]\n",
    "ids_train = ids_all[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting data to dataframes...\n",
      "writing the data to .csv files...\n",
      "data has been processed and written to .csv files in ../data/\n"
     ]
    }
   ],
   "source": [
    "# convert the data into datarames and then write to .csv files\n",
    "print('converting data to dataframes...')\n",
    "\n",
    "# cross validation data\n",
    "df_cv_cont = pd.DataFrame(X_cv_cont)\n",
    "df_cv_cat = pd.DataFrame(X_cv_cat)\n",
    "df_cv_y = pd.DataFrame(y_cv)\n",
    "df_cv_ids = pd.DataFrame(ids_cv)\n",
    "\n",
    "# test data\n",
    "df_test_cont = pd.DataFrame(X_test_cont)\n",
    "df_test_cat = pd.DataFrame(X_test_cat)\n",
    "df_test_ids = pd.DataFrame(ids_test)\n",
    "\n",
    "# train data\n",
    "df_train_cont = pd.DataFrame(X_train_cont)\n",
    "df_train_cat = pd.DataFrame(X_train_cat)\n",
    "df_train_y = pd.DataFrame(y_train)\n",
    "df_train_ids = pd.DataFrame(ids_train)\n",
    "\n",
    "print('writing the data to .csv files...')\n",
    "\n",
    "# cross validation data\n",
    "df_cv_cont.to_csv(path_or_buf=data_dir+'X_cv_cont.csv')\n",
    "df_cv_cat.to_csv(path_or_buf=data_dir+'X_cv_cat.csv')\n",
    "df_cv_y.to_csv(path_or_buf=data_dir+'y_cv.csv')\n",
    "df_cv_ids.to_csv(path_or_buf=data_dir+'ids_cv.csv')\n",
    "\n",
    "# test data\n",
    "df_test_cont.to_csv(path_or_buf=data_dir+'X_test_cont.csv')\n",
    "df_test_cat.to_csv(path_or_buf=data_dir+'X_test_cat.csv')\n",
    "df_test_ids.to_csv(path_or_buf=data_dir+'ids_test.csv')\n",
    "\n",
    "# train data\n",
    "df_train_cont.to_csv(path_or_buf=data_dir+'X_train_cont.csv')\n",
    "df_train_cat.to_csv(path_or_buf=data_dir+'X_train_cat.csv')\n",
    "df_train_y.to_csv(path_or_buf=data_dir+'y_train.csv')\n",
    "df_train_ids.to_csv(path_or_buf=data_dir+'ids_train.csv')\n",
    "\n",
    "print('data has been processed and written to .csv files in ' + data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "data has been loaded!\n"
     ]
    }
   ],
   "source": [
    "# read in the train dataset from .csv files\n",
    "# (only needed if starting from here and not above)\n",
    "print('loading data...')\n",
    "\n",
    "# cross validation data\n",
    "df_cv_cont = pd.read_csv(data_dir+'X_cv_cont.csv', header=0, index_col=0)\n",
    "df_cv_cat = pd.read_csv(data_dir+'X_cv_cat.csv', header=0, index_col=0)\n",
    "df_cv_y = pd.read_csv(data_dir+'y_cv.csv', header=0, index_col=0)\n",
    "df_cv_ids = pd.read_csv(data_dir+'ids_cv.csv', header=0, index_col=0)\n",
    "\n",
    "# test data\n",
    "df_test_cont = pd.read_csv(data_dir+'X_test_cont.csv', header=0, index_col=0)\n",
    "df_test_cat = pd.read_csv(data_dir+'X_test_cat.csv', header=0, index_col=0)\n",
    "df_test_ids = pd.read_csv(data_dir+'ids_test.csv', header=0, index_col=0)\n",
    "\n",
    "# train data\n",
    "df_train_cont = pd.read_csv(data_dir+'X_train_cont.csv', header=0, index_col=0)\n",
    "df_train_cat = pd.read_csv(data_dir+'X_train_cat.csv', header=0, index_col=0)\n",
    "df_train_y = pd.read_csv(data_dir+'y_train.csv', header=0, index_col=0)\n",
    "df_train_ids = pd.read_csv(data_dir+'ids_train.csv', header=0, index_col=0)\n",
    "print('data has been loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a model...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# get values from the dataframes\n",
    "X_cv_cont = df_cv_cont.values\n",
    "X_cv_cat = df_cv_cat.values\n",
    "y_cv = np.ravel(df_cv_y.values)\n",
    "\n",
    "# create a basic Random Forest Classifier to use for feature selection\n",
    "print('creating a model...')\n",
    "# create a tree to select features\n",
    "tree_cat = RandomForestRegressor(n_jobs=n_jobs,\n",
    "    random_state=1, n_estimators=100,\n",
    "    max_features='sqrt', max_depth=10)\n",
    "tree_cont = RandomForestRegressor(n_jobs=n_jobs,\n",
    "    random_state=1, n_estimators=100,\n",
    "    max_features='sqrt', max_depth=10)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting features...\n",
      "shape of X_cv_cat after variance threshold\n",
      "(188318, 175)\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "print('selecting features...')\n",
    "\n",
    "# use variance threshold to select features\n",
    "# many of the features are in categories with few vars\n",
    "selector_variance_cat = VarianceThreshold(threshold=0.01)\n",
    "X_cv_cat = selector_variance_cat.fit_transform(X_cv_cat)\n",
    "print('shape of X_cv_cat after variance threshold')\n",
    "print(X_cv_cat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tree to continuous data...\n",
      "[ 4  9  8  7  3  0 12  5  2 10 11  1 13  6]\n",
      "fitting tree to categorical data...\n",
      "[116 148  89 154 141 109 105 157  74 110  79 143  88 145  75  78 136 156\n",
      "  61  44  46  45  26 113  60  80  27  58  54  32  14 155  64  33 111  55\n",
      "  15  81 130  43 134  29 121 122 106 144  65 153  59 108 140 152 107 112\n",
      "  47  98  42 138 104 151  66 133 150 129  67 147 127 142  56 139  92  41\n",
      " 131  70 114 117  40  50  34  71  93 115 149 132 125  49  51 103  30  86\n",
      " 126 146  31  39  57  37  35  48  28  10  36   6  94 128  69  11  68  38\n",
      "  87   8   5   7  20   9  83  82  52 158 101  53  63  62 165  73  91   4\n",
      "  72 170  17  90 162  24 159   2  25  16 166 172 169 164  21 119 168   3\n",
      " 161 135 173 167  84 137   0  85 174  12 120 124   1  18 123 100 171 163\n",
      "  22  19  23  13 102  77  76 118 160  95  97  96  99]\n",
      "finished features selection!\n"
     ]
    }
   ],
   "source": [
    "# fit the model for continuous data\n",
    "print('fitting tree to continuous data...')\n",
    "tree_cont.fit(X_cv_cont, y_cv)\n",
    "feature_importances_cont = tree_cont.feature_importances_\n",
    "feature_mapping_cont = {importance:idx for idx, importance in \\\n",
    "    enumerate(feature_importances_cont)}\n",
    "sorted_features_cont = feature_importances_cont.argsort()\n",
    "sorted_indices_cont = []\n",
    "print(sorted_features_cont)\n",
    "for x in sorted_features_cont[:num_features_cont]:\n",
    "    sorted_indices_cont.insert(0, x)\n",
    "\n",
    "# create a basic tree for categorical features\n",
    "print('fitting tree to categorical data...')\n",
    "tree_cat.fit(X_cv_cat, y_cv)\n",
    "feature_importances_cat = tree_cat.feature_importances_\n",
    "feature_mapping_cat = {importance:idx for idx, importance in \\\n",
    "    enumerate(feature_importances_cat)}\n",
    "sorted_features_cat = feature_importances_cat.argsort()\n",
    "sorted_indices_cat = []\n",
    "print(sorted_features_cat)\n",
    "for x in sorted_features_cat[:num_features_cat]:\n",
    "    sorted_indices_cat.insert(0, x)\n",
    "\n",
    "print('finished features selection!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing output data with selected features...\n",
      "[108, 59, 153, 65, 144, 106, 122, 121, 29, 134, 43, 130, 81, 15, 55, 111, 33, 64, 155, 14, 32, 54, 58, 27, 80, 60, 113, 26, 45, 46, 44, 61, 156, 136, 78, 75, 145, 88, 143, 79, 110, 74, 157, 105, 109, 141, 154, 89, 148, 116]\n",
      "(188318, 301)\n",
      "df_cv_cat_sel head\n",
      "   108   59  153   65  144  106  122  121   29  134 ...   110   74  157  105  \\\n",
      "0  1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0 ...   1.0  1.0  0.0  0.0   \n",
      "1  1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0 ...   1.0  1.0  0.0  0.0   \n",
      "2  1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0 ...   1.0  1.0  0.0  0.0   \n",
      "3  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0 ...   1.0  1.0  0.0  0.0   \n",
      "4  1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0 ...   1.0  1.0  0.0  0.0   \n",
      "\n",
      "   109  141  154   89  148  116  \n",
      "0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "df_cv_cat_sel column sum\n",
      "108    188173.0\n",
      "59       3558.0\n",
      "153    181347.0\n",
      "65        957.0\n",
      "144    154275.0\n",
      "106    183762.0\n",
      "122    188273.0\n",
      "121       722.0\n",
      "29         34.0\n",
      "134    188176.0\n",
      "43         43.0\n",
      "130    179982.0\n",
      "81       7141.0\n",
      "15      11044.0\n",
      "55       7380.0\n",
      "111       182.0\n",
      "33       1309.0\n",
      "64     187361.0\n",
      "155       788.0\n",
      "14     177274.0\n",
      "32     187009.0\n",
      "54     180938.0\n",
      "58     184760.0\n",
      "27       2277.0\n",
      "80     181177.0\n",
      "60     182980.0\n",
      "113      3022.0\n",
      "26     186041.0\n",
      "45      30873.0\n",
      "46     181977.0\n",
      "44     157445.0\n",
      "61       5338.0\n",
      "156        49.0\n",
      "136    188011.0\n",
      "78     180119.0\n",
      "75      18995.0\n",
      "145     34017.0\n",
      "88     183991.0\n",
      "143     69996.0\n",
      "79       8199.0\n",
      "110    188136.0\n",
      "74     169323.0\n",
      "157       358.0\n",
      "105     15369.0\n",
      "109       145.0\n",
      "141      9672.0\n",
      "154      6183.0\n",
      "89       4327.0\n",
      "148      3561.0\n",
      "116    188018.0\n",
      "dtype: float64\n",
      "df_cv_y head\n",
      "         0\n",
      "0  2213.18\n",
      "1  1283.60\n",
      "2  3005.09\n",
      "3   939.85\n",
      "4  2763.85\n",
      "df_test_cat_sel head\n",
      "   108   59  153   65  144  106  122  121   29  134 ...   110   74  157  105  \\\n",
      "0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0 ...   0.0  0.0  0.0  1.0   \n",
      "1  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0 ...   0.0  0.0  0.0  1.0   \n",
      "2  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0 ...   0.0  1.0  0.0  1.0   \n",
      "3  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0 ...   0.0  0.0  0.0  1.0   \n",
      "4  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0 ...   0.0  0.0  0.0  1.0   \n",
      "\n",
      "   109  141  154   89  148  116  \n",
      "0  1.0  1.0  0.0  1.0  0.0  0.0  \n",
      "1  1.0  1.0  0.0  1.0  1.0  0.0  \n",
      "2  1.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3  1.0  1.0  0.0  1.0  1.0  0.0  \n",
      "4  1.0  1.0  0.0  1.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "successfully wrote selected features to file!\n"
     ]
    }
   ],
   "source": [
    "print('writing output data with selected features...')\n",
    "\n",
    "# cross validation\n",
    "df_cv_cont_sel = df_cv_cont.iloc[:, sorted_indices_cont[:num_features_cont]]\n",
    "df_cv_cont_sel.to_csv(path_or_buf=xgboost_data_dir+'X_cv_cont_sel.csv')\n",
    "df_cv_cat_sel = df_cv_cat.iloc[:, sorted_indices_cat[:num_features_cat]]\n",
    "df_cv_cat_sel.to_csv(path_or_buf=xgboost_data_dir+'X_cv_cat_sel.csv')\n",
    "\n",
    "# test\n",
    "df_test_cont_sel = df_test_cont.iloc[:, sorted_indices_cont[:num_features_cont]]\n",
    "df_test_cont_sel.to_csv(path_or_buf=xgboost_data_dir+'X_test_cont_sel.csv')\n",
    "df_test_cat_sel = df_test_cat.iloc[:, sorted_indices_cat[:num_features_cat]]\n",
    "df_test_cat_sel.to_csv(path_or_buf=xgboost_data_dir+'X_test_cat_sel.csv')\n",
    "\n",
    "#train\n",
    "df_train_cont_sel = df_train_cont.iloc[:, sorted_indices_cont[:num_features_cont]]\n",
    "df_train_cont_sel.to_csv(path_or_buf=xgboost_data_dir+'X_test_cont_sel.csv')\n",
    "print(sorted_indices_cat[:num_features_cat])\n",
    "print(df_train_cat.shape)\n",
    "df_train_cat_sel = df_train_cat.iloc[:, sorted_indices_cat[:num_features_cat]]\n",
    "df_train_cat_sel.to_csv(path_or_buf=xgboost_data_dir+'X_test_cat_sel.csv')\n",
    "\n",
    "# write the data to svmlight files that can be used by xgboost\n",
    "\n",
    "# cross validation\n",
    "X_cv_cont_sel = df_cv_cont_sel.values\n",
    "dump_svmlight_file(X_cv_cont_sel, y_cv, xgboost_data_dir+'cv_cont_sel.dat',\n",
    "                   zero_based=True, multilabel=False)\n",
    "\n",
    "# train\n",
    "X_train_cont_sel = df_train_cont_sel.values\n",
    "dump_svmlight_file(X_train_cont_sel, y_train, xgboost_data_dir+'X_train_cont_sel.dat',\n",
    "                    zero_based=True, multilabel=False)\n",
    "\n",
    "# test\n",
    "X_test_cont_sel = df_test_cont_sel.values\n",
    "# make up y_test because it does not exist\n",
    "y_test = [ 0 for x in range(X_test_cont_sel.shape[0])]\n",
    "dump_svmlight_file(X_test_cont_sel, y_test, xgboost_data_dir+'X_test_cont_sel.dat',\n",
    "                   zero_based=True, multilabel=False)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "X_cv_cat_sel = df_cv_cat_sel.values\n",
    "# some checks to see status\n",
    "print('df_cv_cat_sel head')\n",
    "print(df_cv_cat_sel.head())\n",
    "print('df_cv_cat_sel column sum')\n",
    "print(df_cv_cat_sel.sum(axis=0))\n",
    "print('df_cv_y head')\n",
    "print(df_cv_y.head())\n",
    "dump_svmlight_file(X_cv_cat_sel, y_cv, xgboost_data_dir+'cv_cat_sel.dat',\n",
    "                   zero_based=True, multilabel=False)\n",
    "\n",
    "# train\n",
    "X_train_cat_sel = df_train_cat_sel.values\n",
    "dump_svmlight_file(X_train_cat_sel, y_train, xgboost_data_dir+'X_train_cat_sel.dat',\n",
    "                   zero_based=True, multilabel=False)\n",
    "\n",
    "# test\n",
    "X_test_cat_sel = df_test_cat_sel.values\n",
    "print('df_test_cat_sel head')\n",
    "print(df_test_cat_sel.head())\n",
    "dump_svmlight_file(X_test_cat_sel, y_test, xgboost_data_dir+'X_test_cat_sel.dat',\n",
    "                   zero_based=True, multilabel=False)\n",
    "\n",
    "print('successfully wrote selected features to file!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paramlist for xgboost\n",
    "param = {}\n",
    "param['objective'] = 'reg:linear'\n",
    "param['nthread'] = 2\n",
    "param['eval_metric'] = 'mae'\n",
    "# maximum depth of tree\n",
    "param['max_depth'] = 10  #3-10\n",
    "# analogous to learning rate\n",
    "param['eta'] = 0.05  #0.01-0.02\n",
    "# larger values prevent overfitting\n",
    "param['min_child_weight'] = 0.3  #0-1\n",
    "param['silent'] = 1  # prints messages to screen (1 silences these)\n",
    "param['tree_method'] = 'auto'\n",
    "param['lambda'] = 0  #0-1\n",
    "param['alpha'] = 0  #0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation using categorical xgboost model...\n",
      "    test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
      "0     2885.043050     16.779644     2885.056966       8.111193\n",
      "1     2741.287923     16.463348     2741.263835       7.689561\n",
      "2     2606.070800     16.099239     2605.879232       7.290617\n",
      "3     2480.235596     15.731681     2479.781087       6.894811\n",
      "4     2365.227213     15.301017     2364.294271       6.526983\n",
      "5     2261.316569     14.896306     2259.838542       6.176213\n",
      "6     2168.536296     14.447310     2166.397949       5.875583\n",
      "7     2086.390950     14.078405     2083.548421       5.619960\n",
      "8     2014.153442     13.600379     2010.526042       5.404990\n",
      "9     1950.998210     13.132238     1946.467814       5.211249\n",
      "10    1896.078125     12.680925     1890.621216       4.983472\n",
      "11    1848.434814     12.307592     1842.058390       4.800816\n",
      "12    1807.278157     11.978721     1799.890259       4.694602\n",
      "13    1771.799520     11.726861     1763.399821       4.506312\n",
      "14    1741.332112     11.470806     1731.927205       4.380512\n",
      "15    1715.271240     11.212054     1704.814372       4.278067\n",
      "16    1692.936564     10.945666     1681.435791       4.149210\n",
      "17    1673.883870     10.527914     1661.445963       4.117981\n",
      "18    1657.828491     10.137770     1644.443848       3.973746\n",
      "19    1644.390015      9.729419     1630.014120       3.958943\n",
      "20    1633.203979      9.286451     1617.865356       3.931294\n",
      "21    1623.952148      9.009773     1607.706543       3.801976\n",
      "22    1616.547160      8.740259     1599.396647       3.696872\n",
      "23    1610.590373      8.436026     1592.513509       3.653417\n",
      "24    1605.852702      8.136159     1586.931275       3.636469\n",
      "25    1602.295736      7.826500     1582.490804       3.629341\n",
      "26    1599.656738      7.641322     1579.036377       3.585421\n",
      "27    1597.799846      7.496138     1576.416260       3.520475\n",
      "28    1596.737508      7.276746     1574.564535       3.597660\n",
      "29    1596.251058      7.088064     1573.275391       3.575170\n",
      "30    1596.209920      6.895393     1572.442953       3.544628\n",
      "training with all cat train data...\n",
      "predicting outputs...\n",
      "[ 2650.16870117  2675.30908203  8134.87304688 ...,  2041.875       1732.9942627\n",
      "  3543.04882812]\n"
     ]
    }
   ],
   "source": [
    "# train the categorical model for the data\n",
    "\n",
    "print('cross validation using categorical xgboost model...')\n",
    "data_cv_cat = xgb.DMatrix(xgboost_data_dir+'cv_cat_sel.dat')\n",
    "evaluations_cat = xgb.cv(params=param, dtrain=data_cv_cat, num_boost_round=num_boost_rounds,\n",
    "            nfold=n_folds, early_stopping_rounds=early_stopping_rounds)\n",
    "print(evaluations_cat)\n",
    "\n",
    "print('training with all cat train data...')\n",
    "data_train_cat = xgb.DMatrix(xgboost_data_dir+'X_train_cat_sel.dat')\n",
    "tree_cat = xgb.train( param, data_train_cat, num_boost_rounds)\n",
    "tree_cat.save_model('categorical.model')\n",
    "\n",
    "print('predicting outputs...')\n",
    "data_test_cat = xgb.DMatrix(xgboost_data_dir+'X_test_cat_sel.dat')\n",
    "y_pred_cat = tree_cat.predict(data_test_cat)\n",
    "print(y_pred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation using continuous xgboost model...\n",
      "    test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
      "0     2885.000733     17.566056     2885.046387       8.116454\n",
      "1     2741.687826     18.068665     2741.249105       7.719913\n",
      "2     2607.395589     18.369722     2606.254639       7.323146\n",
      "3     2484.220378     18.568338     2481.954508       7.019232\n",
      "4     2374.279541     18.497003     2370.353027       6.772639\n",
      "5     2278.070231     18.172985     2272.374186       6.669026\n",
      "6     2195.375325     17.988749     2187.642659       6.516154\n",
      "7     2124.711914     17.628771     2114.935872       6.451837\n",
      "8     2064.680054     17.190060     2052.587443       6.537306\n",
      "9     2013.584839     16.457641     1999.315877       6.571406\n",
      "10    1970.574056     15.895278     1953.770223       6.502162\n",
      "11    1934.475179     15.548049     1915.096232       6.522559\n",
      "12    1904.349650     14.972430     1882.288656       6.541310\n",
      "13    1879.015503     14.537666     1854.568237       6.619546\n",
      "14    1858.338867     14.040965     1831.412679       6.766027\n",
      "15    1841.341512     13.556511     1812.003377       6.896574\n",
      "16    1827.610881     13.089555     1795.866089       6.960817\n",
      "17    1816.812093     12.731751     1782.637044       7.051294\n",
      "18    1808.238240     12.352657     1771.739176       7.115704\n",
      "19    1801.625244     12.082654     1762.920573       7.146879\n",
      "20    1796.812581     11.722753     1755.794922       7.315504\n",
      "21    1793.506144     11.457527     1750.241781       7.318747\n",
      "22    1791.458537     11.191411     1745.946370       7.490970\n",
      "23    1790.493856     10.930365     1742.804850       7.600847\n",
      "24    1790.397420     10.704533     1740.643799       7.642596\n",
      "training with all continous train data...\n",
      "predicting outputs...\n",
      "[ 2069.64111328  3533.1171875   3439.07592773 ...,  3853.07421875\n",
      "  3015.40087891  2388.20629883]\n"
     ]
    }
   ],
   "source": [
    "# train the continuous model for the data\n",
    "\n",
    "print('cross validation using continuous xgboost model...')\n",
    "data_cv_cont = xgb.DMatrix(xgboost_data_dir+'cv_cont_sel.dat')\n",
    "evaluations_cont = xgb.cv(params=param, dtrain=data_cv_cont, num_boost_round=num_boost_rounds,\n",
    "            nfold=n_folds, early_stopping_rounds=early_stopping_rounds)\n",
    "print(evaluations_cont)\n",
    "\n",
    "print('training with all continous train data...')\n",
    "data_train_cont = xgb.DMatrix(xgboost_data_dir+'X_train_cont_sel.dat')\n",
    "tree_cont = xgb.train( param, data_train_cont, num_boost_rounds)\n",
    "tree_cont.save_model('continuous.model')\n",
    "\n",
    "print('predicting outputs...')\n",
    "data_test_cont = xgb.DMatrix(xgboost_data_dir+'X_test_cont_sel.dat')\n",
    "y_pred_cont = tree_cont.predict(data_test_cont)\n",
    "print(y_pred_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2359.90478516  3104.21313477  5786.97460938 ...,  2947.47460938\n",
      "  2374.19750977  2965.62744141]\n",
      "[     4      6      9 ..., 587627 587629 587634]\n"
     ]
    }
   ],
   "source": [
    "# Average the outputs together and write the to .csv files\n",
    "\n",
    "test_pred = (y_pred_cat + y_pred_cont) / 2\n",
    "print(test_pred)\n",
    "print(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing output to allstateClaims_combined.csv...\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "# write output to a file\n",
    "outputFile = 'allstateClaims_combined.csv'\n",
    "print('writing output to %s...' % outputFile)\n",
    "prediction_file = open(outputFile, 'w')\n",
    "open_file_object = csv.writer(prediction_file)\n",
    "open_file_object.writerow(['id', 'loss'])\n",
    "open_file_object.writerows(zip(ids_test, test_pred))\n",
    "prediction_file.close()\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all prediction data...\n",
      "[ 1994.87121582  2295.13256836  2442.53393555 ...,  2353.89086914\n",
      "  3014.96923828  1895.92468262]\n",
      "[ 20294.671875    16088.92382812   5645.06201172 ...,  16088.92382812\n",
      "  16088.92382812  16088.92382812]\n",
      "[     4      6      9 ..., 587627 587629 587634]\n",
      "[ 11144.77154541   9192.02819824   4043.79797363 ...,   9221.40734863\n",
      "   9551.9465332    8992.42425537]\n",
      "writing output to ../allStateClaims_combination.csv...\n",
      "completed!\n"
     ]
    }
   ],
   "source": [
    "# average the outputs together\n",
    "print('loading all prediction data...')\n",
    "\n",
    "df_out_cont = pd.read_csv(xgboost_data_dir+'allstateClaims_cont.csv', header=0, index_col=0)\n",
    "df_out_cat = pd.read_csv(xgboost_data_dir+'allstateClaims_cat.csv', header=0, index_col=0)\n",
    "df_out_ids = pd.read_csv(data_dir+'ids_test.csv', header=0, index_col=0)\n",
    "\n",
    "# average the output of the two columns\n",
    "out_cont = df_out_cont.values\n",
    "print(out_cont[:,0])\n",
    "out_cat = df_out_cat.values\n",
    "print(out_cat[:, 0])\n",
    "out_ids = df_out_ids.values\n",
    "print(out_ids[:, 0])\n",
    "\n",
    "test_pred = (out_cont + out_cat) / 2.0\n",
    "print(test_pred[:, 0])\n",
    "\n",
    "# write the output to csv\n",
    "outputFile = '../allStateClaims_combination.csv'\n",
    "print('writing output to %s...' % outputFile)\n",
    "prediction_file = open(outputFile, 'w')\n",
    "open_file_object = csv.writer(prediction_file)\n",
    "open_file_object.writerow(['id', 'loss'])\n",
    "open_file_object.writerows(zip(out_ids[:, 0], test_pred[:, 0]))\n",
    "prediction_file.close()\n",
    "print('completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
